{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting openai==1.11.1\n",
      "  Using cached openai-1.11.1-py3-none-any.whl (226 kB)\n",
      "Requirement already satisfied: tiktoken in /home/sosa.s/.local/lib/python3.10/site-packages (0.5.2)\n",
      "Requirement already satisfied: litellm in /home/sosa.s/.local/lib/python3.10/site-packages (0.7.10)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in /home/sosa.s/.local/lib/python3.10/site-packages (from openai==1.11.1) (4.2.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in /home/sosa.s/.local/lib/python3.10/site-packages (from openai==1.11.1) (1.9.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in /home/sosa.s/.local/lib/python3.10/site-packages (from openai==1.11.1) (0.26.0)\n",
      "Requirement already satisfied: pydantic<3,>=1.9.0 in /home/sosa.s/.local/lib/python3.10/site-packages (from openai==1.11.1) (2.6.1)\n",
      "Requirement already satisfied: sniffio in /home/sosa.s/.local/lib/python3.10/site-packages (from openai==1.11.1) (1.3.0)\n",
      "Requirement already satisfied: tqdm>4 in /apps/pytorch/2.0.1/lib/python3.10/site-packages (from openai==1.11.1) (4.66.1)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.7 in /home/sosa.s/.local/lib/python3.10/site-packages (from openai==1.11.1) (4.9.0)\n",
      "Requirement already satisfied: regex>=2022.1.18 in /home/sosa.s/.local/lib/python3.10/site-packages (from tiktoken) (2023.10.3)\n",
      "Requirement already satisfied: requests>=2.26.0 in /home/sosa.s/.local/lib/python3.10/site-packages (from tiktoken) (2.29.0)\n",
      "Requirement already satisfied: appdirs<2.0.0,>=1.4.4 in /home/sosa.s/.local/lib/python3.10/site-packages (from litellm) (1.4.4)\n",
      "Requirement already satisfied: certifi<2024.0.0,>=2023.7.22 in /apps/pytorch/2.0.1/lib/python3.10/site-packages (from litellm) (2023.11.17)\n",
      "Requirement already satisfied: click in /apps/pytorch/2.0.1/lib/python3.10/site-packages (from litellm) (8.1.7)\n",
      "Requirement already satisfied: importlib-metadata>=6.8.0 in /home/sosa.s/.local/lib/python3.10/site-packages (from litellm) (7.0.1)\n",
      "Requirement already satisfied: jinja2<4.0.0,>=3.1.2 in /apps/pytorch/2.0.1/lib/python3.10/site-packages (from litellm) (3.1.3)\n",
      "INFO: pip is looking at multiple versions of litellm to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting litellm\n",
      "  Downloading litellm-1.22.5-py3-none-any.whl (3.2 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.2/3.2 MB\u001b[0m \u001b[31m44.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: aiohttp in /apps/pytorch/2.0.1/lib/python3.10/site-packages (from litellm) (3.8.6)\n",
      "Requirement already satisfied: python-dotenv>=0.2.0 in /home/sosa.s/.local/lib/python3.10/site-packages (from litellm) (1.0.1)\n",
      "Collecting requests>=2.26.0 (from tiktoken)\n",
      "  Downloading requests-2.31.0-py3-none-any.whl (62 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.6/62.6 kB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: tokenizers in /home/sosa.s/.local/lib/python3.10/site-packages (from litellm) (0.15.1)\n",
      "Requirement already satisfied: idna>=2.8 in /apps/pytorch/2.0.1/lib/python3.10/site-packages (from anyio<5,>=3.5.0->openai==1.11.1) (3.4)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.2 in /home/sosa.s/.local/lib/python3.10/site-packages (from anyio<5,>=3.5.0->openai==1.11.1) (1.2.0)\n",
      "Requirement already satisfied: httpcore==1.* in /home/sosa.s/.local/lib/python3.10/site-packages (from httpx<1,>=0.23.0->openai==1.11.1) (1.0.2)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /home/sosa.s/.local/lib/python3.10/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai==1.11.1) (0.14.0)\n",
      "Requirement already satisfied: zipp>=0.5 in /apps/pytorch/2.0.1/lib/python3.10/site-packages (from importlib-metadata>=6.8.0->litellm) (3.15.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /apps/pytorch/2.0.1/lib/python3.10/site-packages (from jinja2<4.0.0,>=3.1.2->litellm) (2.1.2)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in /home/sosa.s/.local/lib/python3.10/site-packages (from pydantic<3,>=1.9.0->openai==1.11.1) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.16.2 in /home/sosa.s/.local/lib/python3.10/site-packages (from pydantic<3,>=1.9.0->openai==1.11.1) (2.16.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /apps/pytorch/2.0.1/lib/python3.10/site-packages (from requests>=2.26.0->tiktoken) (3.1.0)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/sosa.s/.local/lib/python3.10/site-packages (from requests>=2.26.0->tiktoken) (1.26.18)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /apps/pytorch/2.0.1/lib/python3.10/site-packages (from aiohttp->litellm) (23.1.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /apps/pytorch/2.0.1/lib/python3.10/site-packages (from aiohttp->litellm) (6.0.4)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /apps/pytorch/2.0.1/lib/python3.10/site-packages (from aiohttp->litellm) (4.0.3)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /apps/pytorch/2.0.1/lib/python3.10/site-packages (from aiohttp->litellm) (1.9.2)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /apps/pytorch/2.0.1/lib/python3.10/site-packages (from aiohttp->litellm) (1.4.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /apps/pytorch/2.0.1/lib/python3.10/site-packages (from aiohttp->litellm) (1.3.1)\n",
      "Requirement already satisfied: huggingface_hub<1.0,>=0.16.4 in /home/sosa.s/.local/lib/python3.10/site-packages (from tokenizers->litellm) (0.20.3)\n",
      "Requirement already satisfied: filelock in /apps/pytorch/2.0.1/lib/python3.10/site-packages (from huggingface_hub<1.0,>=0.16.4->tokenizers->litellm) (3.13.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /apps/pytorch/2.0.1/lib/python3.10/site-packages (from huggingface_hub<1.0,>=0.16.4->tokenizers->litellm) (2023.12.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /apps/pytorch/2.0.1/lib/python3.10/site-packages (from huggingface_hub<1.0,>=0.16.4->tokenizers->litellm) (6.0.1)\n",
      "Requirement already satisfied: packaging>=20.9 in /apps/pytorch/2.0.1/lib/python3.10/site-packages (from huggingface_hub<1.0,>=0.16.4->tokenizers->litellm) (23.1)\n",
      "Installing collected packages: requests, openai, litellm\n",
      "  Attempting uninstall: requests\n",
      "    Found existing installation: requests 2.29.0\n",
      "    Uninstalling requests-2.29.0:\n",
      "      Successfully uninstalled requests-2.29.0\n",
      "  Attempting uninstall: openai\n",
      "    Found existing installation: openai 0.28.1\n",
      "    Uninstalling openai-0.28.1:\n",
      "      Successfully uninstalled openai-0.28.1\n",
      "  Attempting uninstall: litellm\n",
      "    Found existing installation: litellm 0.7.10\n",
      "    Uninstalling litellm-0.7.10:\n",
      "      Successfully uninstalled litellm-0.7.10\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "langchain 0.0.178 requires SQLAlchemy<3,>=1.4, which is not installed.\n",
      "langchain 0.0.178 requires tenacity<9.0.0,>=8.1.0, which is not installed.\n",
      "langchain 0.0.178 requires pydantic<2,>=1, but you have pydantic 2.6.1 which is incompatible.\n",
      "openplugin 0.0.31 requires litellm<0.8.0,>=0.7.10, but you have litellm 1.22.5 which is incompatible.\n",
      "openplugin 0.0.31 requires openai<0.28.0,>=0.27.7, but you have openai 1.11.1 which is incompatible.\n",
      "openplugin 0.0.31 requires pydantic<2.0.0,>=1.0.0, but you have pydantic 2.6.1 which is incompatible.\n",
      "openplugin 0.0.31 requires requests==2.29.0, but you have requests 2.31.0 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed litellm-1.22.5 openai-1.11.1 requests-2.31.0\n"
     ]
    }
   ],
   "source": [
    "!pip install openai==1.11.1 tiktoken litellm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hosted inference\n",
    "<img src=\"hosted_inference.JPG\"\n",
    "     alt=\"Markdown Monster icon\"\n",
    "     style=\"float: left; margin-right: 10px;\" />\n",
    "- \n",
    "-\n",
    "## Infrastructure\n",
    "- Azure, AWS, GCP\n",
    "- note on huggingface\n",
    "\n",
    "## AI Research\n",
    "- Training the models\n",
    "- Often times provide endpoints for inference with abstraction for their features\n",
    "\n",
    "## Proxies\n",
    "- Hosted service features\n",
    "- OpenAI client as a standard\n",
    "  - see below how Braintrust does it (https://www.braintrustdata.com/docs/guides/proxy)\n",
    "- HTTP requests\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "import os\n",
    "import time\n",
    " \n",
    "client = OpenAI(\n",
    "  base_url=\"https://braintrustproxy.com/v1\",\n",
    "  api_key=os.environ[\"BRAINTRUST_API_KEY\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [Litellm](https://github.com/BerriAI/litellm)\n",
    "Proxy for accessing other models through the same OpenAI methods and responses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import litellm\n",
    "from litellm import completion\n",
    "litellm.set_verbose = True\n",
    "from openai import OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"HUGGINGFACE_API_KEY\"] = \"\"\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "openai_client = OpenAI(\n",
    "    api_key=os.environ.get(\"OPENAI_API_KEY\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# General Usage\n",
    "- same inputs and outputs (opeai standard)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"What is the weather in Gainesville?\"\n",
    "messages = [{ \"content\": prompt,\"role\": \"user\"}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[92mRequest to litellm:\u001b[0m\n",
      "\u001b[92mlitellm.completion(model='huggingface/mistralai/Mixtral-8x7B-Instruct-v0.1', messages=[{'content': 'What is the weather in Gainesville?', 'role': 'user'}])\u001b[0m\n",
      "\n",
      "\n",
      "self.optional_params: {}\n",
      "kwargs[caching]: False; litellm.cache: None\n",
      "self.optional_params: {}\n",
      "mistralai/Mixtral-8x7B-Instruct-v0.1, text-generation-inference\n",
      "\u001b[92m\n",
      "\n",
      "POST Request Sent from LiteLLM:\n",
      "curl -X POST \\\n",
      "https://api-inference.huggingface.co/models/mistralai/Mixtral-8x7B-Instruct-v0.1 \\\n",
      "-H 'content-type: application/json' -H 'Authorization: Bearer hf_iTujaxiucYAVDE********************' \\\n",
      "-d '{'inputs': '<s>[INST] What is the weather in Gainesville? [/INST]', 'parameters': {'details': True, 'return_full_text': False}, 'stream': False}'\n",
      "\u001b[0m\n",
      "\n",
      "response: [{'generated_text': \" I don't have real-time data or the ability to provide current weather updates. I recommend checking a reliable weather forecasting website or app for the most accurate and up-to-date information on the weather in Gainesville. Gainesville is located in the state of Florida, USA, and is known for its subtropical climate with hot, humid summers and mild, dry winters. However, the weather can vary throughout the year, so it's always\", 'details': {'finish_reason': 'length', 'generated_tokens': 100, 'seed': None, 'prefill': [], 'tokens': [{'id': 315, 'text': ' I', 'logprob': -0.006515503, 'special': False}, {'id': 949, 'text': ' don', 'logprob': -1.5854836e-05, 'special': False}, {'id': 28742, 'text': \"'\", 'logprob': 0.0, 'special': False}, {'id': 28707, 'text': 't', 'logprob': 0.0, 'special': False}, {'id': 506, 'text': ' have', 'logprob': 0.0, 'special': False}, {'id': 1353, 'text': ' real', 'logprob': -2.0503998e-05, 'special': False}, {'id': 28733, 'text': '-', 'logprob': 0.0, 'special': False}, {'id': 1536, 'text': 'time', 'logprob': 0.0, 'special': False}, {'id': 1178, 'text': ' data', 'logprob': -0.35058594, 'special': False}, {'id': 442, 'text': ' or', 'logprob': -3.5762787e-06, 'special': False}, {'id': 272, 'text': ' the', 'logprob': -0.29638672, 'special': False}, {'id': 5537, 'text': ' ability', 'logprob': -4.7683716e-06, 'special': False}, {'id': 298, 'text': ' to', 'logprob': 0.0, 'special': False}, {'id': 3084, 'text': ' provide', 'logprob': -0.015182495, 'special': False}, {'id': 1868, 'text': ' current', 'logprob': -0.29956055, 'special': False}, {'id': 8086, 'text': ' weather', 'logprob': -1.3113022e-06, 'special': False}, {'id': 11756, 'text': ' updates', 'logprob': -0.07849121, 'special': False}, {'id': 28723, 'text': '.', 'logprob': -0.0013456345, 'special': False}, {'id': 315, 'text': ' I', 'logprob': -0.28710938, 'special': False}, {'id': 6557, 'text': ' recommend', 'logprob': -0.08666992, 'special': False}, {'id': 12779, 'text': ' checking', 'logprob': -0.051727295, 'special': False}, {'id': 264, 'text': ' a', 'logprob': -2.503395e-06, 'special': False}, {'id': 13227, 'text': ' reliable', 'logprob': -0.0340271, 'special': False}, {'id': 8086, 'text': ' weather', 'logprob': -0.30517578, 'special': False}, {'id': 19912, 'text': ' forecast', 'logprob': -0.0713501, 'special': False}, {'id': 288, 'text': 'ing', 'logprob': -0.0021591187, 'special': False}, {'id': 4400, 'text': ' website', 'logprob': -0.00040602684, 'special': False}, {'id': 442, 'text': ' or', 'logprob': -1.1920929e-07, 'special': False}, {'id': 954, 'text': ' app', 'logprob': -0.75927734, 'special': False}, {'id': 354, 'text': ' for', 'logprob': -0.121520996, 'special': False}, {'id': 272, 'text': ' the', 'logprob': -0.0030441284, 'special': False}, {'id': 1080, 'text': ' most', 'logprob': -0.018157959, 'special': False}, {'id': 11229, 'text': ' accurate', 'logprob': -0.0014133453, 'special': False}, {'id': 304, 'text': ' and', 'logprob': -0.050109863, 'special': False}, {'id': 582, 'text': ' up', 'logprob': -6.67572e-06, 'special': False}, {'id': 28733, 'text': '-', 'logprob': 0.0, 'special': False}, {'id': 532, 'text': 'to', 'logprob': 0.0, 'special': False}, {'id': 28733, 'text': '-', 'logprob': 0.0, 'special': False}, {'id': 1408, 'text': 'date', 'logprob': -1.1920929e-07, 'special': False}, {'id': 1871, 'text': ' information', 'logprob': -0.05657959, 'special': False}, {'id': 356, 'text': ' on', 'logprob': -0.00983429, 'special': False}, {'id': 272, 'text': ' the', 'logprob': -0.00016605854, 'special': False}, {'id': 8086, 'text': ' weather', 'logprob': -2.670288e-05, 'special': False}, {'id': 297, 'text': ' in', 'logprob': 0.0, 'special': False}, {'id': 420, 'text': ' G', 'logprob': 0.0, 'special': False}, {'id': 17251, 'text': 'aines', 'logprob': 0.0, 'special': False}, {'id': 5485, 'text': 'ville', 'logprob': 0.0, 'special': False}, {'id': 28723, 'text': '.', 'logprob': -0.03805542, 'special': False}, {'id': 420, 'text': ' G', 'logprob': -0.1071167, 'special': False}, {'id': 17251, 'text': 'aines', 'logprob': -7.1525574e-07, 'special': False}, {'id': 5485, 'text': 'ville', 'logprob': 0.0, 'special': False}, {'id': 349, 'text': ' is', 'logprob': -0.016860962, 'special': False}, {'id': 5651, 'text': ' located', 'logprob': -0.0022296906, 'special': False}, {'id': 297, 'text': ' in', 'logprob': 0.0, 'special': False}, {'id': 272, 'text': ' the', 'logprob': -0.4111328, 'special': False}, {'id': 1665, 'text': ' state', 'logprob': -8.952618e-05, 'special': False}, {'id': 302, 'text': ' of', 'logprob': 0.0, 'special': False}, {'id': 9500, 'text': ' Florida', 'logprob': 0.0, 'special': False}, {'id': 28725, 'text': ',', 'logprob': -0.00018012524, 'special': False}, {'id': 7035, 'text': ' USA', 'logprob': -0.33032227, 'special': False}, {'id': 28725, 'text': ',', 'logprob': -0.014389038, 'special': False}, {'id': 304, 'text': ' and', 'logprob': -0.054107666, 'special': False}, {'id': 349, 'text': ' is', 'logprob': -0.11383057, 'special': False}, {'id': 2651, 'text': ' known', 'logprob': -0.00030469894, 'special': False}, {'id': 354, 'text': ' for', 'logprob': -4.541874e-05, 'special': False}, {'id': 871, 'text': ' its', 'logprob': -0.03881836, 'special': False}, {'id': 1083, 'text': ' sub', 'logprob': -0.026794434, 'special': False}, {'id': 28707, 'text': 't', 'logprob': -5.9604645e-07, 'special': False}, {'id': 1506, 'text': 'rop', 'logprob': -2.3841858e-07, 'special': False}, {'id': 745, 'text': 'ical', 'logprob': 0.0, 'special': False}, {'id': 11259, 'text': ' climate', 'logprob': -0.00021791458, 'special': False}, {'id': 395, 'text': ' with', 'logprob': -0.20373535, 'special': False}, {'id': 3296, 'text': ' hot', 'logprob': -0.00014781952, 'special': False}, {'id': 28725, 'text': ',', 'logprob': -0.0004017353, 'special': False}, {'id': 1997, 'text': ' hum', 'logprob': -2.193451e-05, 'special': False}, {'id': 313, 'text': 'id', 'logprob': 0.0, 'special': False}, {'id': 2648, 'text': ' sum', 'logprob': -3.2186508e-06, 'special': False}, {'id': 14448, 'text': 'mers', 'logprob': 0.0, 'special': False}, {'id': 304, 'text': ' and', 'logprob': -1.1920929e-07, 'special': False}, {'id': 16583, 'text': ' mild', 'logprob': -0.0013990402, 'special': False}, {'id': 28725, 'text': ',', 'logprob': -0.6176758, 'special': False}, {'id': 6964, 'text': ' dry', 'logprob': -0.018157959, 'special': False}, {'id': 3108, 'text': ' win', 'logprob': -6.67572e-06, 'special': False}, {'id': 1532, 'text': 'ters', 'logprob': 0.0, 'special': False}, {'id': 28723, 'text': '.', 'logprob': -7.390976e-06, 'special': False}, {'id': 2993, 'text': ' However', 'logprob': -0.003929138, 'special': False}, {'id': 28725, 'text': ',', 'logprob': 0.0, 'special': False}, {'id': 272, 'text': ' the', 'logprob': -0.10534668, 'special': False}, {'id': 8086, 'text': ' weather', 'logprob': -0.7363281, 'special': False}, {'id': 541, 'text': ' can', 'logprob': -5.2452087e-06, 'special': False}, {'id': 11204, 'text': ' vary', 'logprob': -0.015129089, 'special': False}, {'id': 5473, 'text': ' throughout', 'logprob': -1.1972656, 'special': False}, {'id': 272, 'text': ' the', 'logprob': 0.0, 'special': False}, {'id': 879, 'text': ' year', 'logprob': -4.6491623e-06, 'special': False}, {'id': 28725, 'text': ',', 'logprob': -0.013938904, 'special': False}, {'id': 579, 'text': ' so', 'logprob': -0.028411865, 'special': False}, {'id': 378, 'text': ' it', 'logprob': -0.0012874603, 'special': False}, {'id': 28742, 'text': \"'\", 'logprob': -0.0029850006, 'special': False}, {'id': 28713, 'text': 's', 'logprob': -2.9802322e-06, 'special': False}, {'id': 1743, 'text': ' always', 'logprob': -0.7055664, 'special': False}]}}]\n",
      "Looking up model=mistralai/Mixtral-8x7B-Instruct-v0.1 in model_cost_map\n",
      "\n",
      "\n",
      "OpenAI Response:\n",
      " I'm sorry, but as an AI language model, I don't have real-time data. You can check the weather in Gainesville by using a weather website or app, or by asking a voice assistant like Siri or Alexa for the current weather conditions.\n",
      "\n",
      "\n",
      "Litellm Response:\n",
      "  I don't have real-time data or the ability to provide current weather updates. I recommend checking a reliable weather forecasting website or app for the most accurate and up-to-date information on the weather in Gainesville. Gainesville is located in the state of Florida, USA, and is known for its subtropical climate with hot, humid summers and mild, dry winters. However, the weather can vary throughout the year, so it's always\n"
     ]
    }
   ],
   "source": [
    "openai_response = openai_client.chat.completions.create(\n",
    "  model=\"gpt-3.5-turbo\", \n",
    "  messages=messages, \n",
    ")\n",
    "litellm_response = completion(\n",
    "  model=\"huggingface/mistralai/Mixtral-8x7B-Instruct-v0.1\", \n",
    "  messages=messages, \n",
    ")\n",
    "\n",
    "print(\"\\n\\nOpenAI Response:\\n\", openai_response.choices[0].message.content)\n",
    "print(\"\\n\\nLitellm Response:\\n\", litellm_response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- works for usecases other than completions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[92mRequest to litellm:\u001b[0m\n",
      "\u001b[92mlitellm.embedding(model='huggingface/microsoft/codebert-base', input=['What is the weather in Gainesville?'])\u001b[0m\n",
      "\n",
      "\n",
      "self.optional_params: {}\n",
      "kwargs[caching]: False; litellm.cache: None\n",
      "self.optional_params: {}\n",
      "\u001b[92m\n",
      "\n",
      "POST Request Sent from LiteLLM:\n",
      "curl -X POST \\\n",
      "https://api-inference.huggingface.co/models/microsoft/codebert-base \\\n",
      "-H 'content-type: application/json' -H 'Authorization: Bearer hf_iTujaxiucYAVDE********************' \\\n",
      "-d '{'inputs': ['What is the weather in Gainesville?']}'\n",
      "\u001b[0m\n",
      "\n",
      "Looking up model=microsoft/codebert-base in model_cost_map\n",
      "\n",
      " [0.004813834559172392, 0.006503415293991566, 0.007745273411273956, -0.011542744934558868, -0.020771712064743042, 0.007791026495397091, -0.006408642046153545, -0.005248485133051872, -0.004555658902972937, -0.042066313326358795]\n",
      "\n",
      "OpenAI shape (1536,)\n",
      "\n",
      "Litellm shape (768,)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from litellm import embedding\n",
    "import os\n",
    "openai_response = openai_client.embeddings.create(\n",
    "  model=\"text-embedding-ada-002\",\n",
    "  input=[prompt],\n",
    ")\n",
    "litellm_response = embedding(\n",
    "    model='huggingface/microsoft/codebert-base',\n",
    "    input=[prompt]\n",
    ")\n",
    "\n",
    "print(\"\\n\", openai_response.data[0].embedding[:10])\n",
    "print(\"\\nOpenAI shape\", np.array(openai_response.data[0].embedding).shape)\n",
    "print(\"\\nLitellm shape\", np.array(litellm_response.data[0][\"embedding\"]).shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- can do streaming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I don't have real-time data or the ability to provide current weather updates. I recommend checking a reliable weather forecasting website or app for the most accurate and up-to-date information on the weather in Gainesville. Gainesville is located in the state of Florida, USA, and is known for its subtropical climate with hot, humid summers and mild, dry winters. However, the weather can vary throughout the year, so it's alwaysGoes into checking if chunk has hiddden created at param\n",
      "Chunks have a created at hidden param\n",
      "Chunks sorted\n",
      "token_counter messages received: [{'content': 'What is the weather in Gainesville?', 'role': 'user'}]\n",
      "Token Counter - using generic token counter, for model=mistralai/Mixtral-8x7B-Instruct-v0.1\n",
      "LiteLLM: Utils - Counting tokens for OpenAI model=gpt-3.5-turbo\n",
      "Token Counter - using generic token counter, for model=mistralai/Mixtral-8x7B-Instruct-v0.1\n",
      "LiteLLM: Utils - Counting tokens for OpenAI model=gpt-3.5-turbo\n",
      "Looking up model=mistralai/Mixtral-8x7B-Instruct-v0.1 in model_cost_map\n",
      "Goes into checking if chunk has hiddden created at param\n",
      "Chunks have a created at hidden param\n",
      "Chunks sorted\n",
      "token_counter messages received: [{'content': 'What is the weather in Gainesville?', 'role': 'user'}]\n",
      "Token Counter - using generic token counter, for model=mistralai/Mixtral-8x7B-Instruct-v0.1\n",
      "LiteLLM: Utils - Counting tokens for OpenAI model=gpt-3.5-turbo\n",
      "Token Counter - using generic token counter, for model=mistralai/Mixtral-8x7B-Instruct-v0.1\n",
      "LiteLLM: Utils - Counting tokens for OpenAI model=gpt-3.5-turbo\n",
      "Looking up model=mistralai/Mixtral-8x7B-Instruct-v0.1 in model_cost_map\n"
     ]
    }
   ],
   "source": [
    "litellm.set_verbose = False\n",
    "\n",
    "stream_response = completion(\n",
    "  model=\"huggingface/mistralai/Mixtral-8x7B-Instruct-v0.1\", \n",
    "  messages=[{ \"content\": prompt, \"role\": \"user\"}], \n",
    "  stream=True,\n",
    ")\n",
    "\n",
    "for chunk in stream_response:\n",
    "  print(chunk.choices[0].delta.content, end=\"\")\n",
    "\n",
    "litellm.set_verbose = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- How was it instant?\n",
    "- What is contained in the chunks of the stream (full generatio or tokens)?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- By superimposing to the prompt the model is able to exhibit functionalities of other LLMs\n",
    "https://github.com/BerriAI/litellm/blob/d69edac11ba4acdb03116cde253cc0d7caadcf68/litellm/llms/prompt_templates/factory.py#L531-L545"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[92mRequest to litellm:\u001b[0m\n",
      "\u001b[92mlitellm.completion(model='huggingface/mistralai/Mixtral-8x7B-Instruct-v0.1', messages=[{'content': 'Answer in spanish', 'role': 'system'}, {'content': 'Produce the function call response and nothing else, here is the prompt:What is the weather in Gainesville?', 'role': 'user'}], functions=[{'name': 'get_current_weather', 'description': 'Get the current weather in a given location', 'parameters': {'type': 'object', 'properties': {'location': {'type': 'string', 'description': 'The city and state, e.g. San Francisco, CA'}, 'unit': {'type': 'string', 'enum': ['celsius', 'fahrenheit']}}, 'required': ['location']}}], temperature=0.01)\u001b[0m\n",
      "\n",
      "\n",
      "self.optional_params: {}\n",
      "kwargs[caching]: False; litellm.cache: None\n",
      "self.optional_params: {'temperature': 0.01}\n",
      "mistralai/Mixtral-8x7B-Instruct-v0.1, text-generation-inference\n",
      "\u001b[92m\n",
      "\n",
      "POST Request Sent from LiteLLM:\n",
      "curl -X POST \\\n",
      "https://api-inference.huggingface.co/models/mistralai/Mixtral-8x7B-Instruct-v0.1 \\\n",
      "-H 'content-type: application/json' -H 'Authorization: Bearer hf_iTujaxiucYAVDE********************' \\\n",
      "-d '{'inputs': \"<s>[INST] Answer in spanishProduce JSON OUTPUT ONLY! The following functions are available to you:\\n{'name': 'get_current_weather', 'description': 'Get the current weather in a given location', 'parameters': {'type': 'object', 'properties': {'location': {'type': 'string', 'description': 'The city and state, e.g. San Francisco, CA'}, 'unit': {'type': 'string', 'enum': ['celsius', 'fahrenheit']}}, 'required': ['location']}}\\n [/INST]</s>[INST] Produce the function call response and nothing else, here is the prompt:What is the weather in Gainesville? [/INST]\", 'parameters': {'temperature': 0.01, 'details': True, 'return_full_text': False}, 'stream': False}'\n",
      "\u001b[0m\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "response: [{'generated_text': ' {\\n\"functions\": [\\n{\\n\"name\": \"get\\\\_current\\\\_weather\",\\n\"parameters\": {\\n\"location\": \"Gainesville\",\\n\"unit\": \"fahrenheit\"\\n}\\n}\\n]\\n}', 'details': {'finish_reason': 'eos_token', 'generated_tokens': 52, 'seed': 7651620485520914323, 'prefill': [], 'tokens': [{'id': 371, 'text': ' {', 'logprob': 0.0, 'special': False}, {'id': 13, 'text': '\\n', 'logprob': 0.0, 'special': False}, {'id': 28739, 'text': '\"', 'logprob': 0.0, 'special': False}, {'id': 19659, 'text': 'functions', 'logprob': 0.0, 'special': False}, {'id': 1264, 'text': '\":', 'logprob': 0.0, 'special': False}, {'id': 733, 'text': ' [', 'logprob': 0.0, 'special': False}, {'id': 13, 'text': '\\n', 'logprob': 0.0, 'special': False}, {'id': 28751, 'text': '{', 'logprob': 0.0, 'special': False}, {'id': 13, 'text': '\\n', 'logprob': 0.0, 'special': False}, {'id': 28739, 'text': '\"', 'logprob': 0.0, 'special': False}, {'id': 861, 'text': 'name', 'logprob': 0.0, 'special': False}, {'id': 1264, 'text': '\":', 'logprob': 0.0, 'special': False}, {'id': 345, 'text': ' \"', 'logprob': 0.0, 'special': False}, {'id': 527, 'text': 'get', 'logprob': 0.0, 'special': False}, {'id': 14048, 'text': '\\\\_', 'logprob': 0.0, 'special': False}, {'id': 3022, 'text': 'current', 'logprob': 0.0, 'special': False}, {'id': 14048, 'text': '\\\\_', 'logprob': 0.0, 'special': False}, {'id': 769, 'text': 'we', 'logprob': 0.0, 'special': False}, {'id': 1223, 'text': 'ather', 'logprob': 0.0, 'special': False}, {'id': 548, 'text': '\",', 'logprob': 0.0, 'special': False}, {'id': 13, 'text': '\\n', 'logprob': 0.0, 'special': False}, {'id': 28739, 'text': '\"', 'logprob': 0.0, 'special': False}, {'id': 11438, 'text': 'parameters', 'logprob': 0.0, 'special': False}, {'id': 1264, 'text': '\":', 'logprob': 0.0, 'special': False}, {'id': 371, 'text': ' {', 'logprob': 0.0, 'special': False}, {'id': 13, 'text': '\\n', 'logprob': 0.0, 'special': False}, {'id': 28739, 'text': '\"', 'logprob': 0.0, 'special': False}, {'id': 2733, 'text': 'location', 'logprob': 0.0, 'special': False}, {'id': 1264, 'text': '\":', 'logprob': 0.0, 'special': False}, {'id': 345, 'text': ' \"', 'logprob': 0.0, 'special': False}, {'id': 28777, 'text': 'G', 'logprob': 0.0, 'special': False}, {'id': 17251, 'text': 'aines', 'logprob': 0.0, 'special': False}, {'id': 5485, 'text': 'ville', 'logprob': 0.0, 'special': False}, {'id': 548, 'text': '\",', 'logprob': 0.0, 'special': False}, {'id': 13, 'text': '\\n', 'logprob': 0.0, 'special': False}, {'id': 28739, 'text': '\"', 'logprob': 0.0, 'special': False}, {'id': 5306, 'text': 'unit', 'logprob': 0.0, 'special': False}, {'id': 1264, 'text': '\":', 'logprob': 0.0, 'special': False}, {'id': 345, 'text': ' \"', 'logprob': 0.0, 'special': False}, {'id': 28722, 'text': 'f', 'logprob': 0.0, 'special': False}, {'id': 18657, 'text': 'ahren', 'logprob': 0.0, 'special': False}, {'id': 12307, 'text': 'heit', 'logprob': 0.0, 'special': False}, {'id': 28739, 'text': '\"', 'logprob': 0.0, 'special': False}, {'id': 13, 'text': '\\n', 'logprob': 0.0, 'special': False}, {'id': 28752, 'text': '}', 'logprob': 0.0, 'special': False}, {'id': 13, 'text': '\\n', 'logprob': 0.0, 'special': False}, {'id': 28752, 'text': '}', 'logprob': 0.0, 'special': False}, {'id': 13, 'text': '\\n', 'logprob': 0.0, 'special': False}, {'id': 28793, 'text': ']', 'logprob': 0.0, 'special': False}, {'id': 13, 'text': '\\n', 'logprob': 0.0, 'special': False}, {'id': 28752, 'text': '}', 'logprob': 0.0, 'special': False}, {'id': 2, 'text': '</s>', 'logprob': 0.0, 'special': True}]}}]\n",
      "Looking up model=mistralai/Mixtral-8x7B-Instruct-v0.1 in model_cost_map\n",
      "\n",
      "\n",
      "content:  {\n",
      "\"functions\": [\n",
      "{\n",
      "\"name\": \"get\\_current\\_weather\",\n",
      "\"parameters\": {\n",
      "\"location\": \"Gainesville\",\n",
      "\"unit\": \"fahrenheit\"\n",
      "}\n",
      "}\n",
      "]\n",
      "}\n",
      "json_content: {\n",
      "  \"functions\": [\n",
      "    {\n",
      "      \"name\": \"get_current_weather\",\n",
      "      \"parameters\": {\n",
      "        \"location\": \"Gainesville\",\n",
      "        \"unit\": \"fahrenheit\"\n",
      "      }\n",
      "    }\n",
      "  ]\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "litellm.add_function_to_prompt = True \n",
    "prompt_engineered_messages = [\n",
    "  { \"content\": \"Answer in spanish\", \"role\": \"system\"},\n",
    "  { \"content\": \"Produce the function call response and nothing else, here is the prompt:\"+prompt,\"role\": \"user\"}\n",
    "]\n",
    "response = completion(\n",
    "  model=\"huggingface/mistralai/Mixtral-8x7B-Instruct-v0.1\", \n",
    "  messages= prompt_engineered_messages,\n",
    "  functions = [\n",
    "    {\n",
    "      \"name\": \"get_current_weather\",\n",
    "      \"description\": \"Get the current weather in a given location\",\n",
    "      \"parameters\": {\n",
    "        \"type\": \"object\",\n",
    "        \"properties\": {\n",
    "          \"location\": {\n",
    "            \"type\": \"string\",\n",
    "            \"description\": \"The city and state, e.g. San Francisco, CA\"\n",
    "          },\n",
    "          \"unit\": {\n",
    "            \"type\": \"string\",\n",
    "            \"enum\": [\"celsius\", \"fahrenheit\"]\n",
    "          }\n",
    "        },\n",
    "        \"required\": [\"location\"]\n",
    "      }\n",
    "    }\n",
    "  ],\n",
    "  temperature=0.01\n",
    ")\n",
    "content = response.choices[0].message.content\n",
    "print(\"\\n\\ncontent:\", content)\n",
    "import json\n",
    "content = response.choices[0].message.content\n",
    "\n",
    "\n",
    "modified_content = content.replace('\\_', '_') # This is a workaround for the Mixtral generation\n",
    "json_content = json.loads(modified_content)\n",
    "print(\"json_content:\", json.dumps(json_content, indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[92mRequest to litellm:\u001b[0m\n",
      "\u001b[92mlitellm.completion(model='huggingface/tiiuae/falcon-7b-instruct', messages=[{'content': \"Answer in spanishProduce JSON OUTPUT ONLY! The following functions are available to you:\\n{'name': 'get_current_weather', 'description': 'Get the current weather in a given location', 'parameters': {'type': 'object', 'properties': {'location': {'type': 'string', 'description': 'The city and state, e.g. San Francisco, CA'}, 'unit': {'type': 'string', 'enum': ['celsius', 'fahrenheit']}}, 'required': ['location']}}\\n\", 'role': 'system'}, {'content': 'Produce the function call response and nothing else, here is the prompt:What is the weather in Gainesville?', 'role': 'user'}], functions=[{'name': 'get_current_weather', 'description': 'Get the current weather in a given location', 'parameters': {'type': 'object', 'properties': {'location': {'type': 'string', 'description': 'The city and state, e.g. San Francisco, CA'}, 'unit': {'type': 'string', 'enum': ['celsius', 'fahrenheit']}}, 'required': ['location']}}])\u001b[0m\n",
      "\n",
      "\n",
      "self.optional_params: {}\n",
      "kwargs[caching]: False; litellm.cache: None\n",
      "self.optional_params: {}\n",
      "tiiuae/falcon-7b-instruct, text-generation-inference\n",
      "\u001b[92m\n",
      "\n",
      "POST Request Sent from LiteLLM:\n",
      "curl -X POST \\\n",
      "https://api-inference.huggingface.co/models/tiiuae/falcon-7b-instruct \\\n",
      "-H 'content-type: application/json' -H 'Authorization: Bearer hf_iTujaxiucYAVDE********************' \\\n",
      "-d '{'inputs': \"Answer in spanishProduce JSON OUTPUT ONLY! The following functions are available to you:\\n{'name': 'get_current_weather', 'description': 'Get the current weather in a given location', 'parameters': {'type': 'object', 'properties': {'location': {'type': 'string', 'description': 'The city and state, e.g. San Francisco, CA'}, 'unit': {'type': 'string', 'enum': ['celsius', 'fahrenheit']}}, 'required': ['location']}}\\nProduce JSON OUTPUT ONLY! The following functions are available to you:\\n{'name': 'get_current_weather', 'description': 'Get the current weather in a given location', 'parameters': {'type': 'object', 'properties': {'location': {'type': 'string', 'description': 'The city and state, e.g. San Francisco, CA'}, 'unit': {'type': 'string', 'enum': ['celsius', 'fahrenheit']}}, 'required': ['location']}}\\nuser:Produce the function call response and nothing else, here is the prompt:What is the weather in Gainesville?\\n\\n\", 'parameters': {'details': True, 'return_full_text': False}, 'stream': False}'\n",
      "\u001b[0m\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "response: [{'generated_text': '{\"name\": \"get_current_weather\", \"description\": \"Get the current weather in a given location\", \"parameters\": {\"type\": \"object\", \"properties\": {\"location\": {\"type\": \"string\", \"description\": \"The city and state, e.g. Gainesville, FL\"}, \"unit\": {\"type\": \"string\", \"enum\": [\"celsius\", \"fahrenheit\"]', 'details': {'finish_reason': 'length', 'generated_tokens': 100, 'seed': None, 'prefill': [], 'tokens': [{'id': 14616, 'text': '{\"', 'logprob': -1.2255859, 'special': False}, {'id': 2827, 'text': 'name', 'logprob': -0.87939453, 'special': False}, {'id': 4153, 'text': '\":', 'logprob': -0.33398438, 'special': False}, {'id': 204, 'text': ' ', 'logprob': -0.008956909, 'special': False}, {'id': 13, 'text': '\"', 'logprob': -0.02192688, 'special': False}, {'id': 710, 'text': 'get', 'logprob': -0.21435547, 'special': False}, {'id': 74, 'text': '_', 'logprob': -0.040130615, 'special': False}, {'id': 9134, 'text': 'current', 'logprob': -0.014251709, 'special': False}, {'id': 74, 'text': '_', 'logprob': -0.0049057007, 'special': False}, {'id': 33922, 'text': 'weather', 'logprob': -0.013198853, 'special': False}, {'id': 1439, 'text': '\",', 'logprob': -0.09692383, 'special': False}, {'id': 204, 'text': ' ', 'logprob': -0.10369873, 'special': False}, {'id': 13, 'text': '\"', 'logprob': -0.004699707, 'special': False}, {'id': 17970, 'text': 'description', 'logprob': -0.11767578, 'special': False}, {'id': 4153, 'text': '\":', 'logprob': -0.010223389, 'special': False}, {'id': 204, 'text': ' ', 'logprob': -0.004699707, 'special': False}, {'id': 13, 'text': '\"', 'logprob': -0.04724121, 'special': False}, {'id': 2951, 'text': 'Get', 'logprob': -0.038635254, 'special': False}, {'id': 248, 'text': ' the', 'logprob': -0.03765869, 'special': False}, {'id': 1539, 'text': ' current', 'logprob': -0.008560181, 'special': False}, {'id': 5015, 'text': ' weather', 'logprob': -0.004470825, 'special': False}, {'id': 272, 'text': ' in', 'logprob': -0.0052375793, 'special': False}, {'id': 241, 'text': ' a', 'logprob': -0.084106445, 'special': False}, {'id': 2132, 'text': ' given', 'logprob': -0.007232666, 'special': False}, {'id': 3584, 'text': ' location', 'logprob': -0.0060920715, 'special': False}, {'id': 1439, 'text': '\",', 'logprob': -0.070129395, 'special': False}, {'id': 204, 'text': ' ', 'logprob': -0.021331787, 'special': False}, {'id': 13, 'text': '\"', 'logprob': -0.0053367615, 'special': False}, {'id': 30012, 'text': 'parameters', 'logprob': -0.73583984, 'special': False}, {'id': 4153, 'text': '\":', 'logprob': -0.004737854, 'special': False}, {'id': 204, 'text': ' ', 'logprob': -0.0107421875, 'special': False}, {'id': 14616, 'text': '{\"', 'logprob': -0.032104492, 'special': False}, {'id': 3270, 'text': 'type', 'logprob': -0.77441406, 'special': False}, {'id': 4153, 'text': '\":', 'logprob': -0.005168915, 'special': False}, {'id': 204, 'text': ' ', 'logprob': -0.0022201538, 'special': False}, {'id': 13, 'text': '\"', 'logprob': -0.05593872, 'special': False}, {'id': 8509, 'text': 'object', 'logprob': -0.23510742, 'special': False}, {'id': 1439, 'text': '\",', 'logprob': -0.03778076, 'special': False}, {'id': 204, 'text': ' ', 'logprob': -0.0046806335, 'special': False}, {'id': 13, 'text': '\"', 'logprob': -0.0028839111, 'special': False}, {'id': 29408, 'text': 'properties', 'logprob': -0.10015869, 'special': False}, {'id': 4153, 'text': '\":', 'logprob': -0.001906395, 'special': False}, {'id': 204, 'text': ' ', 'logprob': -0.0345459, 'special': False}, {'id': 14616, 'text': '{\"', 'logprob': -0.0057640076, 'special': False}, {'id': 17746, 'text': 'location', 'logprob': -0.08673096, 'special': False}, {'id': 4153, 'text': '\":', 'logprob': -0.004825592, 'special': False}, {'id': 204, 'text': ' ', 'logprob': -0.09118652, 'special': False}, {'id': 14616, 'text': '{\"', 'logprob': -0.008956909, 'special': False}, {'id': 3270, 'text': 'type', 'logprob': -0.08911133, 'special': False}, {'id': 4153, 'text': '\":', 'logprob': -0.004753113, 'special': False}, {'id': 204, 'text': ' ', 'logprob': -0.00092458725, 'special': False}, {'id': 13, 'text': '\"', 'logprob': -0.0050849915, 'special': False}, {'id': 4351, 'text': 'string', 'logprob': -0.017791748, 'special': False}, {'id': 1439, 'text': '\",', 'logprob': -0.043792725, 'special': False}, {'id': 204, 'text': ' ', 'logprob': -0.0010519028, 'special': False}, {'id': 13, 'text': '\"', 'logprob': -0.0013599396, 'special': False}, {'id': 17970, 'text': 'description', 'logprob': -0.017196655, 'special': False}, {'id': 4153, 'text': '\":', 'logprob': -0.0034656525, 'special': False}, {'id': 204, 'text': ' ', 'logprob': -0.0012760162, 'special': False}, {'id': 13, 'text': '\"', 'logprob': -0.0090408325, 'special': False}, {'id': 487, 'text': 'The', 'logprob': -0.014137268, 'special': False}, {'id': 2295, 'text': ' city', 'logprob': -0.005153656, 'special': False}, {'id': 273, 'text': ' and', 'logprob': -0.0018005371, 'special': False}, {'id': 1592, 'text': ' state', 'logprob': -0.0019226074, 'special': False}, {'id': 23, 'text': ',', 'logprob': -0.018722534, 'special': False}, {'id': 293, 'text': ' e', 'logprob': -0.0038471222, 'special': False}, {'id': 25, 'text': '.', 'logprob': -0.00045371056, 'special': False}, {'id': 82, 'text': 'g', 'logprob': -0.00033807755, 'special': False}, {'id': 25, 'text': '.', 'logprob': -0.0022258759, 'special': False}, {'id': 52446, 'text': ' Gaines', 'logprob': -0.46728516, 'special': False}, {'id': 3931, 'text': 'ville', 'logprob': -0.00018906593, 'special': False}, {'id': 23, 'text': ',', 'logprob': -0.009002686, 'special': False}, {'id': 11243, 'text': ' FL', 'logprob': -0.06921387, 'special': False}, {'id': 22023, 'text': '\"},', 'logprob': -0.4711914, 'special': False}, {'id': 204, 'text': ' ', 'logprob': -0.0070381165, 'special': False}, {'id': 13, 'text': '\"', 'logprob': -0.0079193115, 'special': False}, {'id': 13953, 'text': 'unit', 'logprob': -0.0021972656, 'special': False}, {'id': 4153, 'text': '\":', 'logprob': -0.0013885498, 'special': False}, {'id': 204, 'text': ' ', 'logprob': -0.014274597, 'special': False}, {'id': 14616, 'text': '{\"', 'logprob': -0.0057754517, 'special': False}, {'id': 3270, 'text': 'type', 'logprob': -0.0033187866, 'special': False}, {'id': 4153, 'text': '\":', 'logprob': -0.0006079674, 'special': False}, {'id': 204, 'text': ' ', 'logprob': -0.0006608963, 'special': False}, {'id': 13, 'text': '\"', 'logprob': -0.002439499, 'special': False}, {'id': 4351, 'text': 'string', 'logprob': -0.012435913, 'special': False}, {'id': 1439, 'text': '\",', 'logprob': -0.0065231323, 'special': False}, {'id': 204, 'text': ' ', 'logprob': -0.0008378029, 'special': False}, {'id': 13, 'text': '\"', 'logprob': -0.00093746185, 'special': False}, {'id': 27758, 'text': 'enum', 'logprob': -0.007320404, 'special': False}, {'id': 4153, 'text': '\":', 'logprob': -0.0011806488, 'special': False}, {'id': 204, 'text': ' ', 'logprob': -0.0013132095, 'special': False}, {'id': 8074, 'text': '[\"', 'logprob': -0.18847656, 'special': False}, {'id': 78, 'text': 'c', 'logprob': -0.059814453, 'special': False}, {'id': 39211, 'text': 'elsius', 'logprob': -0.0025367737, 'special': False}, {'id': 1439, 'text': '\",', 'logprob': -0.015327454, 'special': False}, {'id': 204, 'text': ' ', 'logprob': -0.00067281723, 'special': False}, {'id': 13, 'text': '\"', 'logprob': -0.0010652542, 'special': False}, {'id': 32945, 'text': 'fahren', 'logprob': -0.005870819, 'special': False}, {'id': 11067, 'text': 'heit', 'logprob': -0.00069761276, 'special': False}, {'id': 10022, 'text': '\"]', 'logprob': -0.020309448, 'special': False}]}}]\n",
      "Looking up model=tiiuae/falcon-7b-instruct in model_cost_map\n",
      "\n",
      "\n",
      "content: {\"name\": \"get_current_weather\", \"description\": \"Get the current weather in a given location\", \"parameters\": {\"type\": \"object\", \"properties\": {\"location\": {\"type\": \"string\", \"description\": \"The city and state, e.g. Gainesville, FL\"}, \"unit\": {\"type\": \"string\", \"enum\": [\"celsius\", \"fahrenheit\"]\n"
     ]
    },
    {
     "ename": "JSONDecodeError",
     "evalue": "Expecting ',' delimiter: line 1 column 294 (char 293)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mJSONDecodeError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 34\u001b[0m\n\u001b[1;32m     30\u001b[0m content \u001b[38;5;241m=\u001b[39m response\u001b[38;5;241m.\u001b[39mchoices[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mmessage\u001b[38;5;241m.\u001b[39mcontent\n\u001b[1;32m     33\u001b[0m modified_content \u001b[38;5;241m=\u001b[39m content\u001b[38;5;241m.\u001b[39mreplace(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124m_\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m---> 34\u001b[0m json_content \u001b[38;5;241m=\u001b[39m \u001b[43mjson\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mloads\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodified_content\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     35\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mjson_content:\u001b[39m\u001b[38;5;124m\"\u001b[39m, json\u001b[38;5;241m.\u001b[39mdumps(json_content, indent\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m))\n",
      "File \u001b[0;32m/apps/pytorch/2.0.1/lib/python3.10/json/__init__.py:346\u001b[0m, in \u001b[0;36mloads\u001b[0;34m(s, cls, object_hook, parse_float, parse_int, parse_constant, object_pairs_hook, **kw)\u001b[0m\n\u001b[1;32m    341\u001b[0m     s \u001b[38;5;241m=\u001b[39m s\u001b[38;5;241m.\u001b[39mdecode(detect_encoding(s), \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msurrogatepass\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    343\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\u001b[38;5;28mcls\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m object_hook \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m\n\u001b[1;32m    344\u001b[0m         parse_int \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m parse_float \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m\n\u001b[1;32m    345\u001b[0m         parse_constant \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m object_pairs_hook \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m kw):\n\u001b[0;32m--> 346\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_default_decoder\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    347\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mcls\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    348\u001b[0m     \u001b[38;5;28mcls\u001b[39m \u001b[38;5;241m=\u001b[39m JSONDecoder\n",
      "File \u001b[0;32m/apps/pytorch/2.0.1/lib/python3.10/json/decoder.py:337\u001b[0m, in \u001b[0;36mJSONDecoder.decode\u001b[0;34m(self, s, _w)\u001b[0m\n\u001b[1;32m    332\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecode\u001b[39m(\u001b[38;5;28mself\u001b[39m, s, _w\u001b[38;5;241m=\u001b[39mWHITESPACE\u001b[38;5;241m.\u001b[39mmatch):\n\u001b[1;32m    333\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Return the Python representation of ``s`` (a ``str`` instance\u001b[39;00m\n\u001b[1;32m    334\u001b[0m \u001b[38;5;124;03m    containing a JSON document).\u001b[39;00m\n\u001b[1;32m    335\u001b[0m \n\u001b[1;32m    336\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 337\u001b[0m     obj, end \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mraw_decode\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43midx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_w\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mend\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    338\u001b[0m     end \u001b[38;5;241m=\u001b[39m _w(s, end)\u001b[38;5;241m.\u001b[39mend()\n\u001b[1;32m    339\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m end \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mlen\u001b[39m(s):\n",
      "File \u001b[0;32m/apps/pytorch/2.0.1/lib/python3.10/json/decoder.py:353\u001b[0m, in \u001b[0;36mJSONDecoder.raw_decode\u001b[0;34m(self, s, idx)\u001b[0m\n\u001b[1;32m    344\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Decode a JSON document from ``s`` (a ``str`` beginning with\u001b[39;00m\n\u001b[1;32m    345\u001b[0m \u001b[38;5;124;03ma JSON document) and return a 2-tuple of the Python\u001b[39;00m\n\u001b[1;32m    346\u001b[0m \u001b[38;5;124;03mrepresentation and the index in ``s`` where the document ended.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    350\u001b[0m \n\u001b[1;32m    351\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    352\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 353\u001b[0m     obj, end \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscan_once\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43midx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    354\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[1;32m    355\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m JSONDecodeError(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExpecting value\u001b[39m\u001b[38;5;124m\"\u001b[39m, s, err\u001b[38;5;241m.\u001b[39mvalue) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[0;31mJSONDecodeError\u001b[0m: Expecting ',' delimiter: line 1 column 294 (char 293)"
     ]
    }
   ],
   "source": [
    "litellm.add_function_to_prompt = True \n",
    "\n",
    "response = completion(\n",
    "  model=\"huggingface/tiiuae/falcon-7b-instruct\", \n",
    "  messages=prompt_engineered_messages,\n",
    "  functions = [\n",
    "    {\n",
    "      \"name\": \"get_current_weather\",\n",
    "      \"description\": \"Get the current weather in a given location\",\n",
    "      \"parameters\": {\n",
    "        \"type\": \"object\",\n",
    "        \"properties\": {\n",
    "          \"location\": {\n",
    "            \"type\": \"string\",\n",
    "            \"description\": \"The city and state, e.g. San Francisco, CA\"\n",
    "          },\n",
    "          \"unit\": {\n",
    "            \"type\": \"string\",\n",
    "            \"enum\": [\"celsius\", \"fahrenheit\"]\n",
    "          }\n",
    "        },\n",
    "        \"required\": [\"location\"]\n",
    "      }\n",
    "    }\n",
    "  ],\n",
    ")\n",
    "content = response.choices[0].message.content\n",
    "print(\"\\n\\ncontent:\", content)\n",
    "import json\n",
    "content = response.choices[0].message.content\n",
    "\n",
    "\n",
    "modified_content = content.replace('\\_', '_')\n",
    "json_content = json.loads(modified_content)\n",
    "print(\"json_content:\", json.dumps(json_content, indent=2))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PyTorch-2.0.1",
   "language": "python",
   "name": "pytorch-2.0.1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
