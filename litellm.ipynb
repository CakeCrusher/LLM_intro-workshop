{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting openai==1.11.1\n",
      "  Using cached openai-1.11.1-py3-none-any.whl (226 kB)\n",
      "Requirement already satisfied: tiktoken in /home/sosa.s/.local/lib/python3.10/site-packages (0.5.2)\n",
      "Requirement already satisfied: litellm in /home/sosa.s/.local/lib/python3.10/site-packages (0.7.10)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in /home/sosa.s/.local/lib/python3.10/site-packages (from openai==1.11.1) (4.2.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in /home/sosa.s/.local/lib/python3.10/site-packages (from openai==1.11.1) (1.9.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in /home/sosa.s/.local/lib/python3.10/site-packages (from openai==1.11.1) (0.26.0)\n",
      "Requirement already satisfied: pydantic<3,>=1.9.0 in /home/sosa.s/.local/lib/python3.10/site-packages (from openai==1.11.1) (2.6.1)\n",
      "Requirement already satisfied: sniffio in /home/sosa.s/.local/lib/python3.10/site-packages (from openai==1.11.1) (1.3.0)\n",
      "Requirement already satisfied: tqdm>4 in /apps/pytorch/2.0.1/lib/python3.10/site-packages (from openai==1.11.1) (4.66.1)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.7 in /home/sosa.s/.local/lib/python3.10/site-packages (from openai==1.11.1) (4.9.0)\n",
      "Requirement already satisfied: regex>=2022.1.18 in /home/sosa.s/.local/lib/python3.10/site-packages (from tiktoken) (2023.10.3)\n",
      "Requirement already satisfied: requests>=2.26.0 in /home/sosa.s/.local/lib/python3.10/site-packages (from tiktoken) (2.29.0)\n",
      "Requirement already satisfied: appdirs<2.0.0,>=1.4.4 in /home/sosa.s/.local/lib/python3.10/site-packages (from litellm) (1.4.4)\n",
      "Requirement already satisfied: certifi<2024.0.0,>=2023.7.22 in /apps/pytorch/2.0.1/lib/python3.10/site-packages (from litellm) (2023.11.17)\n",
      "Requirement already satisfied: click in /apps/pytorch/2.0.1/lib/python3.10/site-packages (from litellm) (8.1.7)\n",
      "Requirement already satisfied: importlib-metadata>=6.8.0 in /home/sosa.s/.local/lib/python3.10/site-packages (from litellm) (7.0.1)\n",
      "Requirement already satisfied: jinja2<4.0.0,>=3.1.2 in /apps/pytorch/2.0.1/lib/python3.10/site-packages (from litellm) (3.1.3)\n",
      "INFO: pip is looking at multiple versions of litellm to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting litellm\n",
      "  Downloading litellm-1.22.5-py3-none-any.whl (3.2 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.2/3.2 MB\u001b[0m \u001b[31m44.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: aiohttp in /apps/pytorch/2.0.1/lib/python3.10/site-packages (from litellm) (3.8.6)\n",
      "Requirement already satisfied: python-dotenv>=0.2.0 in /home/sosa.s/.local/lib/python3.10/site-packages (from litellm) (1.0.1)\n",
      "Collecting requests>=2.26.0 (from tiktoken)\n",
      "  Downloading requests-2.31.0-py3-none-any.whl (62 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.6/62.6 kB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: tokenizers in /home/sosa.s/.local/lib/python3.10/site-packages (from litellm) (0.15.1)\n",
      "Requirement already satisfied: idna>=2.8 in /apps/pytorch/2.0.1/lib/python3.10/site-packages (from anyio<5,>=3.5.0->openai==1.11.1) (3.4)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.2 in /home/sosa.s/.local/lib/python3.10/site-packages (from anyio<5,>=3.5.0->openai==1.11.1) (1.2.0)\n",
      "Requirement already satisfied: httpcore==1.* in /home/sosa.s/.local/lib/python3.10/site-packages (from httpx<1,>=0.23.0->openai==1.11.1) (1.0.2)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /home/sosa.s/.local/lib/python3.10/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai==1.11.1) (0.14.0)\n",
      "Requirement already satisfied: zipp>=0.5 in /apps/pytorch/2.0.1/lib/python3.10/site-packages (from importlib-metadata>=6.8.0->litellm) (3.15.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /apps/pytorch/2.0.1/lib/python3.10/site-packages (from jinja2<4.0.0,>=3.1.2->litellm) (2.1.2)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in /home/sosa.s/.local/lib/python3.10/site-packages (from pydantic<3,>=1.9.0->openai==1.11.1) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.16.2 in /home/sosa.s/.local/lib/python3.10/site-packages (from pydantic<3,>=1.9.0->openai==1.11.1) (2.16.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /apps/pytorch/2.0.1/lib/python3.10/site-packages (from requests>=2.26.0->tiktoken) (3.1.0)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/sosa.s/.local/lib/python3.10/site-packages (from requests>=2.26.0->tiktoken) (1.26.18)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /apps/pytorch/2.0.1/lib/python3.10/site-packages (from aiohttp->litellm) (23.1.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /apps/pytorch/2.0.1/lib/python3.10/site-packages (from aiohttp->litellm) (6.0.4)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /apps/pytorch/2.0.1/lib/python3.10/site-packages (from aiohttp->litellm) (4.0.3)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /apps/pytorch/2.0.1/lib/python3.10/site-packages (from aiohttp->litellm) (1.9.2)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /apps/pytorch/2.0.1/lib/python3.10/site-packages (from aiohttp->litellm) (1.4.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /apps/pytorch/2.0.1/lib/python3.10/site-packages (from aiohttp->litellm) (1.3.1)\n",
      "Requirement already satisfied: huggingface_hub<1.0,>=0.16.4 in /home/sosa.s/.local/lib/python3.10/site-packages (from tokenizers->litellm) (0.20.3)\n",
      "Requirement already satisfied: filelock in /apps/pytorch/2.0.1/lib/python3.10/site-packages (from huggingface_hub<1.0,>=0.16.4->tokenizers->litellm) (3.13.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /apps/pytorch/2.0.1/lib/python3.10/site-packages (from huggingface_hub<1.0,>=0.16.4->tokenizers->litellm) (2023.12.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /apps/pytorch/2.0.1/lib/python3.10/site-packages (from huggingface_hub<1.0,>=0.16.4->tokenizers->litellm) (6.0.1)\n",
      "Requirement already satisfied: packaging>=20.9 in /apps/pytorch/2.0.1/lib/python3.10/site-packages (from huggingface_hub<1.0,>=0.16.4->tokenizers->litellm) (23.1)\n",
      "Installing collected packages: requests, openai, litellm\n",
      "  Attempting uninstall: requests\n",
      "    Found existing installation: requests 2.29.0\n",
      "    Uninstalling requests-2.29.0:\n",
      "      Successfully uninstalled requests-2.29.0\n",
      "  Attempting uninstall: openai\n",
      "    Found existing installation: openai 0.28.1\n",
      "    Uninstalling openai-0.28.1:\n",
      "      Successfully uninstalled openai-0.28.1\n",
      "  Attempting uninstall: litellm\n",
      "    Found existing installation: litellm 0.7.10\n",
      "    Uninstalling litellm-0.7.10:\n",
      "      Successfully uninstalled litellm-0.7.10\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "langchain 0.0.178 requires SQLAlchemy<3,>=1.4, which is not installed.\n",
      "langchain 0.0.178 requires tenacity<9.0.0,>=8.1.0, which is not installed.\n",
      "langchain 0.0.178 requires pydantic<2,>=1, but you have pydantic 2.6.1 which is incompatible.\n",
      "openplugin 0.0.31 requires litellm<0.8.0,>=0.7.10, but you have litellm 1.22.5 which is incompatible.\n",
      "openplugin 0.0.31 requires openai<0.28.0,>=0.27.7, but you have openai 1.11.1 which is incompatible.\n",
      "openplugin 0.0.31 requires pydantic<2.0.0,>=1.0.0, but you have pydantic 2.6.1 which is incompatible.\n",
      "openplugin 0.0.31 requires requests==2.29.0, but you have requests 2.31.0 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed litellm-1.22.5 openai-1.11.1 requests-2.31.0\n"
     ]
    }
   ],
   "source": [
    "!pip install openai==1.11.1 tiktoken litellm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hosted inference\n",
    "<img src=\"hosted_inference.JPG\"\n",
    "     alt=\"Markdown Monster icon\"\n",
    "     style=\"float: left; margin-right: 10px;\" />\n",
    "- \n",
    "-\n",
    "## Infrastructure\n",
    "- Azure, AWS, GCP\n",
    "- note on huggingface\n",
    "\n",
    "## AI Research\n",
    "- Training the models\n",
    "- Often times provide endpoints for inference with abstraction for their features\n",
    "\n",
    "## Proxies\n",
    "- Hosted service features\n",
    "- OpenAI client as a standard\n",
    "  - see below how Braintrust does it (https://www.braintrustdata.com/docs/guides/proxy)\n",
    "- HTTP requests\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "import os\n",
    "import time\n",
    " \n",
    "client = OpenAI(\n",
    "  base_url=\"https://braintrustproxy.com/v1\",\n",
    "  api_key=os.environ[\"BRAINTRUST_API_KEY\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [Litellm](https://github.com/BerriAI/litellm)\n",
    "Proxy for accessing other models through the same OpenAI methods and responses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import litellm\n",
    "from litellm import completion\n",
    "litellm.set_verbose = True\n",
    "from openai import OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# os.environ[\"HUGGINGFACE_API_KEY\"] = \"\"\n",
    "# os.environ[\"OPENAI_API_KEY\"] = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "openai_client = OpenAI(\n",
    "    api_key=os.environ.get(\"OPENAI_API_KEY\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# General Usage\n",
    "- same inputs and outputs (opeai standard)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"What is the weather in Gainesville?\"\n",
    "messages = [{ \"content\": prompt,\"role\": \"user\"}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[92mRequest to litellm:\u001b[0m\n",
      "\u001b[92mlitellm.completion(model='huggingface/mistralai/Mixtral-8x7B-Instruct-v0.1', messages=[{'content': 'What is the weather in Gainesville?', 'role': 'user'}])\u001b[0m\n",
      "\n",
      "\n",
      "self.optional_params: {}\n",
      "kwargs[caching]: False; litellm.cache: None\n",
      "self.optional_params: {}\n",
      "mistralai/Mixtral-8x7B-Instruct-v0.1, text-generation-inference\n",
      "\u001b[92m\n",
      "\n",
      "POST Request Sent from LiteLLM:\n",
      "curl -X POST \\\n",
      "https://api-inference.huggingface.co/models/mistralai/Mixtral-8x7B-Instruct-v0.1 \\\n",
      "-H 'content-type: application/json' -H 'Authorization: Bearer hf_xjjahKPGcXInSc********************' \\\n",
      "-d '{'inputs': '<s>[INST] What is the weather in Gainesville? [/INST]', 'parameters': {'details': True, 'return_full_text': False}, 'stream': False}'\n",
      "\u001b[0m\n",
      "\n",
      "response: [{'generated_text': \" I don't have real-time data or location tracking capabilities, so I can't provide you with the current weather in Gainesville. However, I can tell you that Gainesville is located in the state of Florida, USA, and its climate is characterized as humid subtropical, with hot, humid summers and mild, dry winters. The city's weather can be quite variable, with occasional cold fronts bringing cooler temperatures in the winter and\", 'details': {'finish_reason': 'length', 'generated_tokens': 100, 'seed': None, 'prefill': [], 'tokens': [{'id': 315, 'text': ' I', 'logprob': -0.006515503, 'special': False}, {'id': 949, 'text': ' don', 'logprob': -1.5854836e-05, 'special': False}, {'id': 28742, 'text': \"'\", 'logprob': 0.0, 'special': False}, {'id': 28707, 'text': 't', 'logprob': 0.0, 'special': False}, {'id': 506, 'text': ' have', 'logprob': 0.0, 'special': False}, {'id': 1353, 'text': ' real', 'logprob': -4.887581e-06, 'special': False}, {'id': 28733, 'text': '-', 'logprob': 0.0, 'special': False}, {'id': 1536, 'text': 'time', 'logprob': 0.0, 'special': False}, {'id': 1178, 'text': ' data', 'logprob': -0.67041016, 'special': False}, {'id': 442, 'text': ' or', 'logprob': -9.298325e-06, 'special': False}, {'id': 4723, 'text': ' location', 'logprob': -0.41479492, 'special': False}, {'id': 15271, 'text': ' tracking', 'logprob': -0.0002734661, 'special': False}, {'id': 16585, 'text': ' capabilities', 'logprob': -0.4802246, 'special': False}, {'id': 28725, 'text': ',', 'logprob': -3.7670135e-05, 'special': False}, {'id': 579, 'text': ' so', 'logprob': -8.4877014e-05, 'special': False}, {'id': 315, 'text': ' I', 'logprob': 0.0, 'special': False}, {'id': 541, 'text': ' can', 'logprob': -0.0009703636, 'special': False}, {'id': 28742, 'text': \"'\", 'logprob': 0.0, 'special': False}, {'id': 28707, 'text': 't', 'logprob': 0.0, 'special': False}, {'id': 3084, 'text': ' provide', 'logprob': -0.032836914, 'special': False}, {'id': 368, 'text': ' you', 'logprob': -0.14562988, 'special': False}, {'id': 395, 'text': ' with', 'logprob': -0.005748749, 'special': False}, {'id': 272, 'text': ' the', 'logprob': -2.5868416e-05, 'special': False}, {'id': 1868, 'text': ' current', 'logprob': -3.5762787e-07, 'special': False}, {'id': 8086, 'text': ' weather', 'logprob': 0.0, 'special': False}, {'id': 297, 'text': ' in', 'logprob': -4.7683716e-07, 'special': False}, {'id': 420, 'text': ' G', 'logprob': 0.0, 'special': False}, {'id': 17251, 'text': 'aines', 'logprob': -5.9604645e-07, 'special': False}, {'id': 5485, 'text': 'ville', 'logprob': 0.0, 'special': False}, {'id': 28723, 'text': '.', 'logprob': -4.887581e-06, 'special': False}, {'id': 2993, 'text': ' However', 'logprob': -5.376339e-05, 'special': False}, {'id': 28725, 'text': ',', 'logprob': 0.0, 'special': False}, {'id': 315, 'text': ' I', 'logprob': -0.045318604, 'special': False}, {'id': 541, 'text': ' can', 'logprob': -5.543232e-05, 'special': False}, {'id': 1912, 'text': ' tell', 'logprob': -0.03366089, 'special': False}, {'id': 368, 'text': ' you', 'logprob': 0.0, 'special': False}, {'id': 369, 'text': ' that', 'logprob': -6.890297e-05, 'special': False}, {'id': 420, 'text': ' G', 'logprob': -1.9073486e-06, 'special': False}, {'id': 17251, 'text': 'aines', 'logprob': -1.5497208e-06, 'special': False}, {'id': 5485, 'text': 'ville', 'logprob': 0.0, 'special': False}, {'id': 349, 'text': ' is', 'logprob': -0.004211426, 'special': False}, {'id': 5651, 'text': ' located', 'logprob': -0.0010442734, 'special': False}, {'id': 297, 'text': ' in', 'logprob': 0.0, 'special': False}, {'id': 272, 'text': ' the', 'logprob': -0.15881348, 'special': False}, {'id': 1665, 'text': ' state', 'logprob': -0.00010740757, 'special': False}, {'id': 302, 'text': ' of', 'logprob': 0.0, 'special': False}, {'id': 9500, 'text': ' Florida', 'logprob': 0.0, 'special': False}, {'id': 28725, 'text': ',', 'logprob': -0.000626564, 'special': False}, {'id': 7035, 'text': ' USA', 'logprob': -0.0003323555, 'special': False}, {'id': 28725, 'text': ',', 'logprob': -0.001367569, 'special': False}, {'id': 304, 'text': ' and', 'logprob': -0.0001039505, 'special': False}, {'id': 871, 'text': ' its', 'logprob': -0.49731445, 'special': False}, {'id': 11259, 'text': ' climate', 'logprob': -0.109558105, 'special': False}, {'id': 349, 'text': ' is', 'logprob': -1.0728836e-06, 'special': False}, {'id': 23100, 'text': ' characterized', 'logprob': -0.11456299, 'special': False}, {'id': 390, 'text': ' as', 'logprob': -0.0010995865, 'special': False}, {'id': 1997, 'text': ' hum', 'logprob': -0.10192871, 'special': False}, {'id': 313, 'text': 'id', 'logprob': 0.0, 'special': False}, {'id': 1083, 'text': ' sub', 'logprob': -2.2649765e-06, 'special': False}, {'id': 28707, 'text': 't', 'logprob': 0.0, 'special': False}, {'id': 1506, 'text': 'rop', 'logprob': 0.0, 'special': False}, {'id': 745, 'text': 'ical', 'logprob': 0.0, 'special': False}, {'id': 28725, 'text': ',', 'logprob': -0.55566406, 'special': False}, {'id': 395, 'text': ' with', 'logprob': -0.013572693, 'special': False}, {'id': 3296, 'text': ' hot', 'logprob': -1.66893e-06, 'special': False}, {'id': 28725, 'text': ',', 'logprob': -0.010093689, 'special': False}, {'id': 1997, 'text': ' hum', 'logprob': -0.005756378, 'special': False}, {'id': 313, 'text': 'id', 'logprob': 0.0, 'special': False}, {'id': 2648, 'text': ' sum', 'logprob': -2.1457672e-06, 'special': False}, {'id': 14448, 'text': 'mers', 'logprob': 0.0, 'special': False}, {'id': 304, 'text': ' and', 'logprob': -4.7683716e-07, 'special': False}, {'id': 16583, 'text': ' mild', 'logprob': -0.0019512177, 'special': False}, {'id': 28725, 'text': ',', 'logprob': -0.07080078, 'special': False}, {'id': 6964, 'text': ' dry', 'logprob': -0.014251709, 'special': False}, {'id': 3108, 'text': ' win', 'logprob': -1.0848045e-05, 'special': False}, {'id': 1532, 'text': 'ters', 'logprob': 0.0, 'special': False}, {'id': 28723, 'text': '.', 'logprob': 0.0, 'special': False}, {'id': 415, 'text': ' The', 'logprob': -0.33374023, 'special': False}, {'id': 2990, 'text': ' city', 'logprob': -0.90625, 'special': False}, {'id': 28742, 'text': \"'\", 'logprob': -0.32202148, 'special': False}, {'id': 28713, 'text': 's', 'logprob': -2.7418137e-06, 'special': False}, {'id': 8086, 'text': ' weather', 'logprob': -0.07788086, 'special': False}, {'id': 541, 'text': ' can', 'logprob': -0.004673004, 'special': False}, {'id': 347, 'text': ' be', 'logprob': -0.6191406, 'special': False}, {'id': 3448, 'text': ' quite', 'logprob': -0.47460938, 'special': False}, {'id': 7860, 'text': ' variable', 'logprob': -0.0021190643, 'special': False}, {'id': 28725, 'text': ',', 'logprob': -0.042999268, 'special': False}, {'id': 395, 'text': ' with', 'logprob': -0.4868164, 'special': False}, {'id': 20636, 'text': ' occasional', 'logprob': -0.3791504, 'special': False}, {'id': 5256, 'text': ' cold', 'logprob': -0.044128418, 'special': False}, {'id': 2778, 'text': ' front', 'logprob': -8.583069e-06, 'special': False}, {'id': 28713, 'text': 's', 'logprob': 0.0, 'special': False}, {'id': 10279, 'text': ' bringing', 'logprob': -0.5493164, 'special': False}, {'id': 5106, 'text': ' cool', 'logprob': -0.039367676, 'special': False}, {'id': 263, 'text': 'er', 'logprob': 0.0, 'special': False}, {'id': 17991, 'text': ' temperatures', 'logprob': -0.06390381, 'special': False}, {'id': 297, 'text': ' in', 'logprob': -0.02494812, 'special': False}, {'id': 272, 'text': ' the', 'logprob': -0.0026359558, 'special': False}, {'id': 8539, 'text': ' winter', 'logprob': -7.1525574e-07, 'special': False}, {'id': 304, 'text': ' and', 'logprob': -0.0085372925, 'special': False}]}}]\n",
      "Looking up model=mistralai/Mixtral-8x7B-Instruct-v0.1 in model_cost_map\n",
      "\n",
      "\n",
      "OpenAI Response:\n",
      " I'm sorry, I am an AI language model and I don't have access to real-time data. Therefore, I cannot provide the current weather in Gainesville. However, you can easily check the weather by using a search engine or a weather website or app to get the most up-to-date information.\n",
      "\n",
      "\n",
      "Litellm Response:\n",
      "  I don't have real-time data or location tracking capabilities, so I can't provide you with the current weather in Gainesville. However, I can tell you that Gainesville is located in the state of Florida, USA, and its climate is characterized as humid subtropical, with hot, humid summers and mild, dry winters. The city's weather can be quite variable, with occasional cold fronts bringing cooler temperatures in the winter and\n"
     ]
    }
   ],
   "source": [
    "openai_response = openai_client.chat.completions.create(\n",
    "  model=\"gpt-3.5-turbo\", \n",
    "  messages=messages, \n",
    ")\n",
    "litellm_response = completion(\n",
    "  model=\"huggingface/mistralai/Mixtral-8x7B-Instruct-v0.1\", \n",
    "  messages=messages, \n",
    ")\n",
    "\n",
    "print(\"\\n\\nOpenAI Response:\\n\", openai_response.choices[0].message.content)\n",
    "print(\"\\n\\nLitellm Response:\\n\", litellm_response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- works for usecases other than completions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[92mRequest to litellm:\u001b[0m\n",
      "\u001b[92mlitellm.embedding(model='huggingface/microsoft/codebert-base', input=['What is the weather in Gainesville?'])\u001b[0m\n",
      "\n",
      "\n",
      "self.optional_params: {}\n",
      "kwargs[caching]: False; litellm.cache: None\n",
      "self.optional_params: {}\n",
      "\u001b[92m\n",
      "\n",
      "POST Request Sent from LiteLLM:\n",
      "curl -X POST \\\n",
      "https://api-inference.huggingface.co/models/microsoft/codebert-base \\\n",
      "-H 'content-type: application/json' -H 'Authorization: Bearer hf_xjjahKPGcXInSc********************' \\\n",
      "-d '{'inputs': ['What is the weather in Gainesville?']}'\n",
      "\u001b[0m\n",
      "\n",
      "Looking up model=microsoft/codebert-base in model_cost_map\n",
      "\n",
      " [0.004790938924998045, 0.0064730034209787846, 0.007729643490165472, -0.011499562300741673, -0.0208261851221323, 0.007814728654921055, -0.00651554623618722, -0.005255633965134621, -0.004542228765785694, -0.04207124933600426]\n",
      "\n",
      "OpenAI shape (1536,)\n",
      "\n",
      "Litellm shape (768,)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from litellm import embedding\n",
    "import os\n",
    "openai_response = openai_client.embeddings.create(\n",
    "  model=\"text-embedding-ada-002\",\n",
    "  input=[prompt],\n",
    ")\n",
    "litellm_response = embedding(\n",
    "    model='huggingface/microsoft/codebert-base',\n",
    "    input=[prompt]\n",
    ")\n",
    "\n",
    "print(\"\\n\", openai_response.data[0].embedding[:10])\n",
    "print(\"\\nOpenAI shape\", np.array(openai_response.data[0].embedding).shape)\n",
    "print(\"\\nLitellm shape\", np.array(litellm_response.data[0][\"embedding\"]).shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- can do streaming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I\n",
      " don\n",
      "'\n",
      "t\n",
      " have\n",
      " real\n",
      "-\n",
      "time\n",
      " data\n",
      " or\n",
      " location\n",
      " tracking\n",
      " capabilities\n",
      ",\n",
      " so\n",
      " I\n",
      " can\n",
      "'\n",
      "t\n",
      " provide\n",
      " you\n",
      " with\n",
      " the\n",
      " current\n",
      " weather\n",
      " in\n",
      " G\n",
      "aines\n",
      "ville\n",
      ".\n",
      " However\n",
      ",\n",
      " I\n",
      " can\n",
      " tell\n",
      " you\n",
      " that\n",
      " G\n",
      "aines\n",
      "ville\n",
      " is\n",
      " located\n",
      " in\n",
      " the\n",
      " state\n",
      " of\n",
      " Florida\n",
      ",\n",
      " USA\n",
      ",\n",
      " and\n",
      " its\n",
      " climate\n",
      " is\n",
      " characterized\n",
      " as\n",
      " hum\n",
      "id\n",
      " sub\n",
      "t\n",
      "rop\n",
      "ical\n",
      ",\n",
      " with\n",
      " hot\n",
      ",\n",
      " hum\n",
      "id\n",
      " sum\n",
      "mers\n",
      " and\n",
      " mild\n",
      ",\n",
      " dry\n",
      " win\n",
      "ters\n",
      ".\n",
      " The\n",
      " city\n",
      "'\n",
      "s\n",
      " weather\n",
      " can\n",
      " be\n",
      " quite\n",
      " variable\n",
      ",\n",
      " with\n",
      " occasional\n",
      " cold\n",
      " front\n",
      "s\n",
      " bringing\n",
      " cool\n",
      "er\n",
      " temperatures\n",
      " in\n",
      " the\n",
      " winter\n",
      " and\n",
      "Goes into checking if chunk has hiddden created at param\n",
      "Chunks have a created at hidden param\n",
      "Chunks sorted\n",
      "token_counter messages received: [{'content': 'What is the weather in Gainesville?', 'role': 'user'}]\n",
      "Token Counter - using generic token counter, for model=mistralai/Mixtral-8x7B-Instruct-v0.1\n",
      "LiteLLM: Utils - Counting tokens for OpenAI model=gpt-3.5-turbo\n",
      "Token Counter - using generic token counter, for model=mistralai/Mixtral-8x7B-Instruct-v0.1\n",
      "LiteLLM: Utils - Counting tokens for OpenAI model=gpt-3.5-turbo\n",
      "Looking up model=mistralai/Mixtral-8x7B-Instruct-v0.1 in model_cost_map\n",
      "Goes into checking if chunk has hiddden created at param\n",
      "Chunks have a created at hidden param\n",
      "Chunks sorted\n",
      "token_counter messages received: [{'content': 'What is the weather in Gainesville?', 'role': 'user'}]\n",
      "Token Counter - using generic token counter, for model=mistralai/Mixtral-8x7B-Instruct-v0.1\n",
      "LiteLLM: Utils - Counting tokens for OpenAI model=gpt-3.5-turbo\n",
      "Token Counter - using generic token counter, for model=mistralai/Mixtral-8x7B-Instruct-v0.1\n",
      "LiteLLM: Utils - Counting tokens for OpenAI model=gpt-3.5-turbo\n",
      "Looking up model=mistralai/Mixtral-8x7B-Instruct-v0.1 in model_cost_map\n"
     ]
    }
   ],
   "source": [
    "litellm.set_verbose = False\n",
    "\n",
    "stream_response = completion(\n",
    "  model=\"huggingface/mistralai/Mixtral-8x7B-Instruct-v0.1\", \n",
    "  messages=[{ \"content\": prompt, \"role\": \"user\"}], \n",
    "  stream=True,\n",
    ")\n",
    "\n",
    "for chunk in stream_response:\n",
    "  print(chunk.choices[0].delta.content, end=\"\\n\")\n",
    "\n",
    "litellm.set_verbose = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- How was it instant?\n",
    "- What is contained in the chunks of the stream (full generatio or tokens)?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- By superimposing to the prompt the model is able to exhibit functionalities of other LLMs\n",
    "https://github.com/BerriAI/litellm/blob/d69edac11ba4acdb03116cde253cc0d7caadcf68/litellm/llms/prompt_templates/factory.py#L531-L545"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[92mRequest to litellm:\u001b[0m\n",
      "\u001b[92mlitellm.completion(model='huggingface/mistralai/Mixtral-8x7B-Instruct-v0.1', messages=[{'content': 'Answer in spanish', 'role': 'system'}, {'content': 'Produce the function call response and nothing else, here is the prompt:What is the weather in Gainesville?', 'role': 'user'}], functions=[{'name': 'get_current_weather', 'description': 'Get the current weather in a given location', 'parameters': {'type': 'object', 'properties': {'location': {'type': 'string', 'description': 'The city and state, e.g. San Francisco, CA'}, 'unit': {'type': 'string', 'enum': ['celsius', 'fahrenheit']}}, 'required': ['location']}}], temperature=0.01)\u001b[0m\n",
      "\n",
      "\n",
      "self.optional_params: {}\n",
      "kwargs[caching]: False; litellm.cache: None\n",
      "self.optional_params: {'temperature': 0.01}\n",
      "mistralai/Mixtral-8x7B-Instruct-v0.1, text-generation-inference\n",
      "\u001b[92m\n",
      "\n",
      "POST Request Sent from LiteLLM:\n",
      "curl -X POST \\\n",
      "https://api-inference.huggingface.co/models/mistralai/Mixtral-8x7B-Instruct-v0.1 \\\n",
      "-H 'content-type: application/json' -H 'Authorization: Bearer hf_xjjahKPGcXInSc********************' \\\n",
      "-d '{'inputs': \"<s>[INST] Answer in spanishProduce JSON OUTPUT ONLY! The following functions are available to you:\\n{'name': 'get_current_weather', 'description': 'Get the current weather in a given location', 'parameters': {'type': 'object', 'properties': {'location': {'type': 'string', 'description': 'The city and state, e.g. San Francisco, CA'}, 'unit': {'type': 'string', 'enum': ['celsius', 'fahrenheit']}}, 'required': ['location']}}\\n [/INST]</s>[INST] Produce the function call response and nothing else, here is the prompt:What is the weather in Gainesville? [/INST]\", 'parameters': {'temperature': 0.01, 'details': True, 'return_full_text': False}, 'stream': False}'\n",
      "\u001b[0m\n",
      "\n",
      "response: [{'generated_text': ' {\\n\"functions\": [\\n{\\n\"name\": \"get\\\\_current\\\\_weather\",\\n\"parameters\": {\\n\"location\": \"Gainesville\",\\n\"unit\": \"fahrenheit\"\\n}\\n}\\n]\\n}', 'details': {'finish_reason': 'eos_token', 'generated_tokens': 52, 'seed': 15131603742476685440, 'prefill': [], 'tokens': [{'id': 371, 'text': ' {', 'logprob': 0.0, 'special': False}, {'id': 13, 'text': '\\n', 'logprob': 0.0, 'special': False}, {'id': 28739, 'text': '\"', 'logprob': 0.0, 'special': False}, {'id': 19659, 'text': 'functions', 'logprob': 0.0, 'special': False}, {'id': 1264, 'text': '\":', 'logprob': 0.0, 'special': False}, {'id': 733, 'text': ' [', 'logprob': 0.0, 'special': False}, {'id': 13, 'text': '\\n', 'logprob': 0.0, 'special': False}, {'id': 28751, 'text': '{', 'logprob': 0.0, 'special': False}, {'id': 13, 'text': '\\n', 'logprob': 0.0, 'special': False}, {'id': 28739, 'text': '\"', 'logprob': 0.0, 'special': False}, {'id': 861, 'text': 'name', 'logprob': 0.0, 'special': False}, {'id': 1264, 'text': '\":', 'logprob': 0.0, 'special': False}, {'id': 345, 'text': ' \"', 'logprob': 0.0, 'special': False}, {'id': 527, 'text': 'get', 'logprob': 0.0, 'special': False}, {'id': 14048, 'text': '\\\\_', 'logprob': 0.0, 'special': False}, {'id': 3022, 'text': 'current', 'logprob': 0.0, 'special': False}, {'id': 14048, 'text': '\\\\_', 'logprob': 0.0, 'special': False}, {'id': 769, 'text': 'we', 'logprob': 0.0, 'special': False}, {'id': 1223, 'text': 'ather', 'logprob': 0.0, 'special': False}, {'id': 548, 'text': '\",', 'logprob': 0.0, 'special': False}, {'id': 13, 'text': '\\n', 'logprob': 0.0, 'special': False}, {'id': 28739, 'text': '\"', 'logprob': 0.0, 'special': False}, {'id': 11438, 'text': 'parameters', 'logprob': 0.0, 'special': False}, {'id': 1264, 'text': '\":', 'logprob': 0.0, 'special': False}, {'id': 371, 'text': ' {', 'logprob': 0.0, 'special': False}, {'id': 13, 'text': '\\n', 'logprob': 0.0, 'special': False}, {'id': 28739, 'text': '\"', 'logprob': 0.0, 'special': False}, {'id': 2733, 'text': 'location', 'logprob': 0.0, 'special': False}, {'id': 1264, 'text': '\":', 'logprob': 0.0, 'special': False}, {'id': 345, 'text': ' \"', 'logprob': 0.0, 'special': False}, {'id': 28777, 'text': 'G', 'logprob': 0.0, 'special': False}, {'id': 17251, 'text': 'aines', 'logprob': 0.0, 'special': False}, {'id': 5485, 'text': 'ville', 'logprob': 0.0, 'special': False}, {'id': 548, 'text': '\",', 'logprob': 0.0, 'special': False}, {'id': 13, 'text': '\\n', 'logprob': 0.0, 'special': False}, {'id': 28739, 'text': '\"', 'logprob': 0.0, 'special': False}, {'id': 5306, 'text': 'unit', 'logprob': 0.0, 'special': False}, {'id': 1264, 'text': '\":', 'logprob': 0.0, 'special': False}, {'id': 345, 'text': ' \"', 'logprob': 0.0, 'special': False}, {'id': 28722, 'text': 'f', 'logprob': 0.0, 'special': False}, {'id': 18657, 'text': 'ahren', 'logprob': 0.0, 'special': False}, {'id': 12307, 'text': 'heit', 'logprob': 0.0, 'special': False}, {'id': 28739, 'text': '\"', 'logprob': 0.0, 'special': False}, {'id': 13, 'text': '\\n', 'logprob': 0.0, 'special': False}, {'id': 28752, 'text': '}', 'logprob': 0.0, 'special': False}, {'id': 13, 'text': '\\n', 'logprob': 0.0, 'special': False}, {'id': 28752, 'text': '}', 'logprob': 0.0, 'special': False}, {'id': 13, 'text': '\\n', 'logprob': 0.0, 'special': False}, {'id': 28793, 'text': ']', 'logprob': 0.0, 'special': False}, {'id': 13, 'text': '\\n', 'logprob': 0.0, 'special': False}, {'id': 28752, 'text': '}', 'logprob': 0.0, 'special': False}, {'id': 2, 'text': '</s>', 'logprob': 0.0, 'special': True}]}}]\n",
      "Looking up model=mistralai/Mixtral-8x7B-Instruct-v0.1 in model_cost_map\n",
      "\n",
      "\n",
      "content:  {\n",
      "\"functions\": [\n",
      "{\n",
      "\"name\": \"get\\_current\\_weather\",\n",
      "\"parameters\": {\n",
      "\"location\": \"Gainesville\",\n",
      "\"unit\": \"fahrenheit\"\n",
      "}\n",
      "}\n",
      "]\n",
      "}\n",
      "json_content: {\n",
      "  \"functions\": [\n",
      "    {\n",
      "      \"name\": \"get_current_weather\",\n",
      "      \"parameters\": {\n",
      "        \"location\": \"Gainesville\",\n",
      "        \"unit\": \"fahrenheit\"\n",
      "      }\n",
      "    }\n",
      "  ]\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "litellm.add_function_to_prompt = True \n",
    "prompt_engineered_messages = [\n",
    "  { \"content\": \"Answer in spanish\", \"role\": \"system\"},\n",
    "  { \"content\": \"Produce the function call response and nothing else, here is the prompt:\"+prompt,\"role\": \"user\"}\n",
    "]\n",
    "response = completion(\n",
    "  model=\"huggingface/mistralai/Mixtral-8x7B-Instruct-v0.1\", \n",
    "  messages= prompt_engineered_messages,\n",
    "  functions = [\n",
    "    {\n",
    "      \"name\": \"get_current_weather\",\n",
    "      \"description\": \"Get the current weather in a given location\",\n",
    "      \"parameters\": {\n",
    "        \"type\": \"object\",\n",
    "        \"properties\": {\n",
    "          \"location\": {\n",
    "            \"type\": \"string\",\n",
    "            \"description\": \"The city and state, e.g. San Francisco, CA\"\n",
    "          },\n",
    "          \"unit\": {\n",
    "            \"type\": \"string\",\n",
    "            \"enum\": [\"celsius\", \"fahrenheit\"]\n",
    "          }\n",
    "        },\n",
    "        \"required\": [\"location\"]\n",
    "      }\n",
    "    }\n",
    "  ],\n",
    "  temperature=0.01\n",
    ")\n",
    "content = response.choices[0].message.content\n",
    "print(\"\\n\\ncontent:\", content)\n",
    "import json\n",
    "content = response.choices[0].message.content\n",
    "\n",
    "\n",
    "modified_content = content.replace('\\_', '_') # This is a workaround for the Mixtral generation\n",
    "json_content = json.loads(modified_content)\n",
    "print(\"json_content:\", json.dumps(json_content, indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[92mRequest to litellm:\u001b[0m\n",
      "\u001b[92mlitellm.completion(model='huggingface/tiiuae/falcon-7b-instruct', messages=[{'content': \"Answer in spanishProduce JSON OUTPUT ONLY! The following functions are available to you:\\n{'name': 'get_current_weather', 'description': 'Get the current weather in a given location', 'parameters': {'type': 'object', 'properties': {'location': {'type': 'string', 'description': 'The city and state, e.g. San Francisco, CA'}, 'unit': {'type': 'string', 'enum': ['celsius', 'fahrenheit']}}, 'required': ['location']}}\\nProduce JSON OUTPUT ONLY! The following functions are available to you:\\n{'name': 'get_current_weather', 'description': 'Get the current weather in a given location', 'parameters': {'type': 'object', 'properties': {'location': {'type': 'string', 'description': 'The city and state, e.g. San Francisco, CA'}, 'unit': {'type': 'string', 'enum': ['celsius', 'fahrenheit']}}, 'required': ['location']}}\\nProduce JSON OUTPUT ONLY! The following functions are available to you:\\n{'name': 'get_current_weather', 'description': 'Get the current weather in a given location', 'parameters': {'type': 'object', 'properties': {'location': {'type': 'string', 'description': 'The city and state, e.g. San Francisco, CA'}, 'unit': {'type': 'string', 'enum': ['celsius', 'fahrenheit']}}, 'required': ['location']}}\\n\", 'role': 'system'}, {'content': 'Produce the function call response and nothing else, here is the prompt:What is the weather in Gainesville?', 'role': 'user'}], functions=[{'name': 'get_current_weather', 'description': 'Get the current weather in a given location', 'parameters': {'type': 'object', 'properties': {'location': {'type': 'string', 'description': 'The city and state, e.g. San Francisco, CA'}, 'unit': {'type': 'string', 'enum': ['celsius', 'fahrenheit']}}, 'required': ['location']}}])\u001b[0m\n",
      "\n",
      "\n",
      "self.optional_params: {}\n",
      "kwargs[caching]: False; litellm.cache: None\n",
      "self.optional_params: {}\n",
      "tiiuae/falcon-7b-instruct, text-generation-inference\n",
      "\u001b[92m\n",
      "\n",
      "POST Request Sent from LiteLLM:\n",
      "curl -X POST \\\n",
      "https://api-inference.huggingface.co/models/tiiuae/falcon-7b-instruct \\\n",
      "-H 'content-type: application/json' -H 'Authorization: Bearer hf_xjjahKPGcXInSc********************' \\\n",
      "-d '{'inputs': \"Answer in spanishProduce JSON OUTPUT ONLY! The following functions are available to you:\\n{'name': 'get_current_weather', 'description': 'Get the current weather in a given location', 'parameters': {'type': 'object', 'properties': {'location': {'type': 'string', 'description': 'The city and state, e.g. San Francisco, CA'}, 'unit': {'type': 'string', 'enum': ['celsius', 'fahrenheit']}}, 'required': ['location']}}\\nProduce JSON OUTPUT ONLY! The following functions are available to you:\\n{'name': 'get_current_weather', 'description': 'Get the current weather in a given location', 'parameters': {'type': 'object', 'properties': {'location': {'type': 'string', 'description': 'The city and state, e.g. San Francisco, CA'}, 'unit': {'type': 'string', 'enum': ['celsius', 'fahrenheit']}}, 'required': ['location']}}\\nProduce JSON OUTPUT ONLY! The following functions are available to you:\\n{'name': 'get_current_weather', 'description': 'Get the current weather in a given location', 'parameters': {'type': 'object', 'properties': {'location': {'type': 'string', 'description': 'The city and state, e.g. San Francisco, CA'}, 'unit': {'type': 'string', 'enum': ['celsius', 'fahrenheit']}}, 'required': ['location']}}\\nProduce JSON OUTPUT ONLY! The following functions are available to you:\\n{'name': 'get_current_weather', 'description': 'Get the current weather in a given location', 'parameters': {'type': 'object', 'properties': {'location': {'type': 'string', 'description': 'The city and state, e.g. San Francisco, CA'}, 'unit': {'type': 'string', 'enum': ['celsius', 'fahrenheit']}}, 'required': ['location']}}\\nuser:Produce the function call response and nothing else, here is the prompt:What is the weather in Gainesville?\\n\\n\", 'parameters': {'details': True, 'return_full_text': False}, 'stream': False}'\n",
      "\u001b[0m\n",
      "\n",
      "response: [{'error': 'Model tiiuae/falcon-7b-instruct is currently loading', 'estimated_time': 1685.13330078125}]\n",
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm.set_verbose=True'.\n",
      "\n",
      "Logging Details: logger_fn - None | callable(logger_fn) - False\n",
      "Logging Details LiteLLM-Failure Call\n",
      "self.failure_callback: []\n"
     ]
    },
    {
     "ename": "APIError",
     "evalue": "HuggingfaceException - response is not in expected format - [{'error': 'Model tiiuae/falcon-7b-instruct is currently loading', 'estimated_time': 1685.13330078125}]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mHuggingfaceError\u001b[0m                          Traceback (most recent call last)",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/litellm/main.py:1215\u001b[0m, in \u001b[0;36mcompletion\u001b[0;34m(model, messages, timeout, temperature, top_p, n, stream, stop, max_tokens, presence_penalty, frequency_penalty, logit_bias, user, response_format, seed, tools, tool_choice, logprobs, top_logprobs, deployment_id, functions, function_call, base_url, api_version, api_key, model_list, **kwargs)\u001b[0m\n\u001b[1;32m   1214\u001b[0m custom_prompt_dict \u001b[38;5;241m=\u001b[39m custom_prompt_dict \u001b[38;5;129;01mor\u001b[39;00m litellm\u001b[38;5;241m.\u001b[39mcustom_prompt_dict\n\u001b[0;32m-> 1215\u001b[0m model_response \u001b[38;5;241m=\u001b[39m \u001b[43mhuggingface\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompletion\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1216\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1217\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmessages\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1218\u001b[0m \u001b[43m    \u001b[49m\u001b[43mapi_base\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mapi_base\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore\u001b[39;49;00m\n\u001b[1;32m   1219\u001b[0m \u001b[43m    \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhf_headers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1220\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel_response\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel_response\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1221\u001b[0m \u001b[43m    \u001b[49m\u001b[43mprint_verbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprint_verbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1222\u001b[0m \u001b[43m    \u001b[49m\u001b[43moptional_params\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptional_params\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1223\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlitellm_params\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlitellm_params\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1224\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlogger_fn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlogger_fn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1225\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1226\u001b[0m \u001b[43m    \u001b[49m\u001b[43mapi_key\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhuggingface_key\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1227\u001b[0m \u001b[43m    \u001b[49m\u001b[43macompletion\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43macompletion\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1228\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlogging_obj\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlogging\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1229\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcustom_prompt_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcustom_prompt_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1230\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1231\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1232\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m   1233\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstream\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m optional_params\n\u001b[1;32m   1234\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m optional_params[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstream\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m==\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m   1235\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m acompletion \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m   1236\u001b[0m ):\n\u001b[1;32m   1237\u001b[0m     \u001b[38;5;66;03m# don't try to access stream object,\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/litellm/llms/huggingface_restapi.py:544\u001b[0m, in \u001b[0;36mHuggingface.completion\u001b[0;34m(self, model, messages, api_base, headers, model_response, print_verbose, timeout, encoding, api_key, logging_obj, custom_prompt_dict, acompletion, optional_params, litellm_params, logger_fn)\u001b[0m\n\u001b[1;32m    543\u001b[0m     exception_mapping_worked \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m--> 544\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[1;32m    545\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/litellm/llms/huggingface_restapi.py:533\u001b[0m, in \u001b[0;36mHuggingface.completion\u001b[0;34m(self, model, messages, api_base, headers, model_response, print_verbose, timeout, encoding, api_key, logging_obj, custom_prompt_dict, acompletion, optional_params, litellm_params, logger_fn)\u001b[0m\n\u001b[1;32m    529\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m HuggingfaceError(\n\u001b[1;32m    530\u001b[0m                 message\u001b[38;5;241m=\u001b[39mcompletion_response[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124merror\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m    531\u001b[0m                 status_code\u001b[38;5;241m=\u001b[39mresponse\u001b[38;5;241m.\u001b[39mstatus_code,\n\u001b[1;32m    532\u001b[0m             )\n\u001b[0;32m--> 533\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconvert_to_model_response_object\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    534\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcompletion_response\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcompletion_response\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    535\u001b[0m \u001b[43m            \u001b[49m\u001b[43mmodel_response\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel_response\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    536\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    537\u001b[0m \u001b[43m            \u001b[49m\u001b[43moptional_params\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptional_params\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    538\u001b[0m \u001b[43m            \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    539\u001b[0m \u001b[43m            \u001b[49m\u001b[43minput_text\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    540\u001b[0m \u001b[43m            \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    541\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    542\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m HuggingfaceError \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/litellm/llms/huggingface_restapi.py:224\u001b[0m, in \u001b[0;36mHuggingface.convert_to_model_response_object\u001b[0;34m(self, completion_response, model_response, task, optional_params, encoding, input_text, model)\u001b[0m\n\u001b[1;32m    219\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m    220\u001b[0m     \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(completion_response, \u001b[38;5;28mlist\u001b[39m)\n\u001b[1;32m    221\u001b[0m     \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(completion_response[\u001b[38;5;241m0\u001b[39m], \u001b[38;5;28mdict\u001b[39m)\n\u001b[1;32m    222\u001b[0m     \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgenerated_text\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m completion_response[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    223\u001b[0m ):\n\u001b[0;32m--> 224\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m HuggingfaceError(\n\u001b[1;32m    225\u001b[0m         status_code\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m422\u001b[39m,\n\u001b[1;32m    226\u001b[0m         message\u001b[38;5;241m=\u001b[39m\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mresponse is not in expected format - \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcompletion_response\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    227\u001b[0m     )\n\u001b[1;32m    229\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(completion_response[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgenerated_text\u001b[39m\u001b[38;5;124m\"\u001b[39m]) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "\u001b[0;31mHuggingfaceError\u001b[0m: response is not in expected format - [{'error': 'Model tiiuae/falcon-7b-instruct is currently loading', 'estimated_time': 1685.13330078125}]",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mAPIError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[17], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m litellm\u001b[38;5;241m.\u001b[39madd_function_to_prompt \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m \n\u001b[0;32m----> 3\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[43mcompletion\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m  \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mhuggingface/tiiuae/falcon-7b-instruct\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m  \u001b[49m\u001b[43mmessages\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprompt_engineered_messages\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m  \u001b[49m\u001b[43mfunctions\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[43m{\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m      \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mname\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mget_current_weather\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m      \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdescription\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mGet the current weather in a given location\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[43m      \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mparameters\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtype\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mobject\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mproperties\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\n\u001b[1;32m     13\u001b[0m \u001b[43m          \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlocation\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\n\u001b[1;32m     14\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtype\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstring\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     15\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdescription\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mThe city and state, e.g. San Francisco, CA\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\n\u001b[1;32m     16\u001b[0m \u001b[43m          \u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     17\u001b[0m \u001b[43m          \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43munit\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\n\u001b[1;32m     18\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtype\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstring\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     19\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43menum\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcelsius\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfahrenheit\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[1;32m     20\u001b[0m \u001b[43m          \u001b[49m\u001b[43m}\u001b[49m\n\u001b[1;32m     21\u001b[0m \u001b[43m        \u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     22\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrequired\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlocation\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[1;32m     23\u001b[0m \u001b[43m      \u001b[49m\u001b[43m}\u001b[49m\n\u001b[1;32m     24\u001b[0m \u001b[43m    \u001b[49m\u001b[43m}\u001b[49m\n\u001b[1;32m     25\u001b[0m \u001b[43m  \u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     26\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m     27\u001b[0m content \u001b[38;5;241m=\u001b[39m response\u001b[38;5;241m.\u001b[39mchoices[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mmessage\u001b[38;5;241m.\u001b[39mcontent\n\u001b[1;32m     28\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mcontent:\u001b[39m\u001b[38;5;124m\"\u001b[39m, content)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/litellm/utils.py:2455\u001b[0m, in \u001b[0;36mclient.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m   2451\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m   2452\u001b[0m             liteDebuggerClient \u001b[38;5;129;01mand\u001b[39;00m liteDebuggerClient\u001b[38;5;241m.\u001b[39mdashboard_url \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   2453\u001b[0m         ):  \u001b[38;5;66;03m# make it easy to get to the debugger logs if you've initialized it\u001b[39;00m\n\u001b[1;32m   2454\u001b[0m             e\u001b[38;5;241m.\u001b[39mmessage \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m Check the log in your dashboard - \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mliteDebuggerClient\u001b[38;5;241m.\u001b[39mdashboard_url\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m-> 2455\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m e\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/litellm/utils.py:2358\u001b[0m, in \u001b[0;36mclient.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m   2356\u001b[0m         print_verbose(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mError while checking max token limit: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mstr\u001b[39m(e)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   2357\u001b[0m \u001b[38;5;66;03m# MODEL CALL\u001b[39;00m\n\u001b[0;32m-> 2358\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43moriginal_function\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2359\u001b[0m end_time \u001b[38;5;241m=\u001b[39m datetime\u001b[38;5;241m.\u001b[39mdatetime\u001b[38;5;241m.\u001b[39mnow()\n\u001b[1;32m   2360\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstream\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m kwargs \u001b[38;5;129;01mand\u001b[39;00m kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstream\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m==\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m:\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/litellm/main.py:1880\u001b[0m, in \u001b[0;36mcompletion\u001b[0;34m(model, messages, timeout, temperature, top_p, n, stream, stop, max_tokens, presence_penalty, frequency_penalty, logit_bias, user, response_format, seed, tools, tool_choice, logprobs, top_logprobs, deployment_id, functions, function_call, base_url, api_version, api_key, model_list, **kwargs)\u001b[0m\n\u001b[1;32m   1877\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m response\n\u001b[1;32m   1878\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m   1879\u001b[0m     \u001b[38;5;66;03m## Map to OpenAI Exception\u001b[39;00m\n\u001b[0;32m-> 1880\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[43mexception_type\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1881\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1882\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcustom_llm_provider\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcustom_llm_provider\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1883\u001b[0m \u001b[43m        \u001b[49m\u001b[43moriginal_exception\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43me\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1884\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcompletion_kwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1885\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/litellm/utils.py:7464\u001b[0m, in \u001b[0;36mexception_type\u001b[0;34m(model, original_exception, custom_llm_provider, completion_kwargs)\u001b[0m\n\u001b[1;32m   7462\u001b[0m \u001b[38;5;66;03m# don't let an error with mapping interrupt the user from receiving an error from the llm api calls\u001b[39;00m\n\u001b[1;32m   7463\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m exception_mapping_worked:\n\u001b[0;32m-> 7464\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[1;32m   7465\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   7466\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m original_exception\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/litellm/utils.py:6921\u001b[0m, in \u001b[0;36mexception_type\u001b[0;34m(model, original_exception, custom_llm_provider, completion_kwargs)\u001b[0m\n\u001b[1;32m   6919\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   6920\u001b[0m             exception_mapping_worked \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m-> 6921\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m APIError(\n\u001b[1;32m   6922\u001b[0m                 status_code\u001b[38;5;241m=\u001b[39moriginal_exception\u001b[38;5;241m.\u001b[39mstatus_code,\n\u001b[1;32m   6923\u001b[0m                 message\u001b[38;5;241m=\u001b[39m\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mHuggingfaceException - \u001b[39m\u001b[38;5;132;01m{\u001b[39;00moriginal_exception\u001b[38;5;241m.\u001b[39mmessage\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   6924\u001b[0m                 llm_provider\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhuggingface\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   6925\u001b[0m                 model\u001b[38;5;241m=\u001b[39mmodel,\n\u001b[1;32m   6926\u001b[0m                 request\u001b[38;5;241m=\u001b[39moriginal_exception\u001b[38;5;241m.\u001b[39mrequest,\n\u001b[1;32m   6927\u001b[0m             )\n\u001b[1;32m   6928\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m custom_llm_provider \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mai21\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m   6929\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(original_exception, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmessage\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "\u001b[0;31mAPIError\u001b[0m: HuggingfaceException - response is not in expected format - [{'error': 'Model tiiuae/falcon-7b-instruct is currently loading', 'estimated_time': 1685.13330078125}]"
     ]
    }
   ],
   "source": [
    "litellm.add_function_to_prompt = True \n",
    "\n",
    "response = completion(\n",
    "  model=\"huggingface/tiiuae/falcon-7b-instruct\", \n",
    "  messages=prompt_engineered_messages,\n",
    "  functions = [\n",
    "    {\n",
    "      \"name\": \"get_current_weather\",\n",
    "      \"description\": \"Get the current weather in a given location\",\n",
    "      \"parameters\": {\n",
    "        \"type\": \"object\",\n",
    "        \"properties\": {\n",
    "          \"location\": {\n",
    "            \"type\": \"string\",\n",
    "            \"description\": \"The city and state, e.g. San Francisco, CA\"\n",
    "          },\n",
    "          \"unit\": {\n",
    "            \"type\": \"string\",\n",
    "            \"enum\": [\"celsius\", \"fahrenheit\"]\n",
    "          }\n",
    "        },\n",
    "        \"required\": [\"location\"]\n",
    "      }\n",
    "    }\n",
    "  ],\n",
    ")\n",
    "content = response.choices[0].message.content\n",
    "print(\"\\n\\ncontent:\", content)\n",
    "import json\n",
    "content = response.choices[0].message.content\n",
    "\n",
    "\n",
    "modified_content = content.replace('\\_', '_')\n",
    "json_content = json.loads(modified_content)\n",
    "print(\"json_content:\", json.dumps(json_content, indent=2))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PyTorch-2.0.1",
   "language": "python",
   "name": "pytorch-2.0.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
