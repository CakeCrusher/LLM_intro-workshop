{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import bfloat16\n",
    "import transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e8f546e5301f42f38160bc09b3ef08bf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "MistralForCausalLM(\n",
       "  (model): MistralModel(\n",
       "    (embed_tokens): Embedding(32000, 4096)\n",
       "    (layers): ModuleList(\n",
       "      (0-31): 32 x MistralDecoderLayer(\n",
       "        (self_attn): MistralAttention(\n",
       "          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "          (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (rotary_emb): MistralRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): MistralMLP(\n",
       "          (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "          (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "          (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): MistralRMSNorm()\n",
       "        (post_attention_layernorm): MistralRMSNorm()\n",
       "      )\n",
       "    )\n",
       "    (norm): MistralRMSNorm()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=4096, out_features=32000, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_id = \"mistralai/Mistral-7B-v0.1\"\n",
    "model = transformers.AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    trust_remote_code=True,\n",
    "    torch_dtype=bfloat16,\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "tokenizer = transformers.AutoTokenizer.from_pretrained(model_id)\n",
    "\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_text = transformers.pipeline(\n",
    "    model=model, tokenizer=tokenizer,\n",
    "    return_full_text=False,  # Set this to True if using langchain\n",
    "    task=\"text-generation\",\n",
    "    temperature=0.1,  # Controls the randomness of outputs\n",
    "    top_p=0.15,  # Probability threshold for selecting tokens\n",
    "    top_k=0,  # Number of top tokens to consider (0 relies on top_p)\n",
    "    max_new_tokens=512,  # Limits the number of generated tokens\n",
    "    repetition_penalty=1.1  # Discourages repetitive outputs\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{align*}\n",
    "    p'_i = p_i^{(1/T)}\n",
    "\\end{align*}\n",
    "<!-- show image temperature_ex.JPG -->\n",
    "<img src=\"temperature_ex.JPG\"\n",
    "     alt=\"Markdown Monster icon\"\n",
    "     style=\"float: left; margin-right: 10px;\" />\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sosa.s/.local/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:392: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "/home/sosa.s/.local/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:397: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.15` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n",
      "/home/sosa.s/.local/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:407: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `0` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.\n",
      "  warnings.warn(\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " bright, and it’s only going to get brighter.\n",
      "\n",
      "## What are the benefits of using AI in business?\n",
      "\n",
      "There are many benefits of using AI in business. Some of these benefits include:\n",
      "\n",
      "1. Increased efficiency and productivity: AI can help businesses automate tasks and processes, which can lead to increased efficiency and productivity.\n",
      "2. Improved decision-making: AI can help businesses make better decisions by providing them with data and insights that they wouldn’t otherwise have access to.\n",
      "3. Enhanced customer experience: AI can help businesses provide a more personalized and tailored experience for their customers.\n",
      "4. Reduced costs: AI can help businesses reduce costs by automating tasks and processes, as well as by making better decisions.\n",
      "5. Competitive advantage: By using AI, businesses can gain a competitive advantage over their competitors.\n",
      "\n",
      "## How can you use AI to improve your business?\n",
      "\n",
      "AI can be used to improve your business in a number of ways. For example, you can use AI to automate tasks, such as customer service or marketing. You can also use AI to analyze data and make predictions about your business. Additionally, AI can be used to create new products or services.\n",
      "\n",
      "## What are some of the challenges of using AI in business?\n",
      "\n",
      "Some of the challenges of using AI in business include:\n",
      "\n",
      "1. Lack of understanding of how AI works: Many people don’t understand how AI works, which can make it difficult to implement in a business setting.\n",
      "2. Data quality issues: In order for AI to work effectively, it needs high-quality data. If the data is not accurate or complete, then the AI will not be able to produce accurate results.\n",
      "3. Ethical concerns: There are ethical concerns surrounding the use of AI, such as privacy and bias. These concerns need to be addressed before implementing AI in a business setting.\n",
      "4. Cost: Implementing AI can be expensive, especially if you need to hire experts to help you set up and maintain the system.\n",
      "5. Security risks: AI systems can be vulnerable to security threats, such as hacking and malware attacks. This is something that businesses need to consider when deciding whether or not to use AI.\n",
      "\n",
      "## What are some of the best practices for using AI in business?\n",
      "\n",
      "There are a few key things to keep in mind when using AI in business. First, it’s important to have\n"
     ]
    }
   ],
   "source": [
    "test_prompt = \"The future of AI is\"\n",
    "result = generate_text(test_prompt)\n",
    "print(result[0]['generated_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fdc2f6b2b3bd4d318314707beecc0db2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/1.46k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1a673d40eaaa403d9f691332ed693eca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.model:   0%|          | 0.00/493k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "afb2ccc1266b4d1495eb69ecd04aefa5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.80M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e321361ea590453fb6755829d9690f60",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/72.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1bfa3434d7b640ea9870a1aedcfce4f8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/596 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b66f805f1eee436884c01f847ee27aa6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors.index.json:   0%|          | 0.00/25.1k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f3bea5d971b64752a8fb73a4a04996eb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "af93f37fb6fc41e7846d0a93ac2d8bbb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00003.safetensors:   0%|          | 0.00/4.94G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a08c6012cc8649e18d785dc2d1cf595e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-00003.safetensors:   0%|          | 0.00/5.00G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "49c85fa5dc884ea0af58cb04efd75203",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00003-of-00003.safetensors:   0%|          | 0.00/4.54G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "64845580b4884216aecb8ee00a7a34c8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a0c56bc812954d3495a5367ff5895e1c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/111 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model_instruct = \"mistralai/Mistral-7B-Instruct-v0.2\"\n",
    "\n",
    "tokenizer_instruct = transformers.AutoTokenizer.from_pretrained(model_instruct)\n",
    "pipeline = transformers.pipeline(\n",
    "    model=model_instruct,\n",
    "    task=\"text-generation\",\n",
    "    model_kwargs={\"torch_dtype\": bfloat16, \"device_map\": \"auto\"},\n",
    ")   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- top_k: selects the k most probable tokens\n",
    "- top_p: selects all tokens with higher than p probability\n",
    "- temperature: selects from the top tokens with .1 favoring the highest probability tokens and .9 having little preference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PROMPT:\n",
      " <s>[INST] The future of AI is [/INST]\n",
      "<s>[INST] The future of AI is [/INST] The future of AI (Artificial Intelligence) is a topic of much debate and speculation among experts, researchers, and industry professionals. Some believe that AI will continue to advance at an exponential rate, leading to significant breakthroughs in areas such as healthcare, education, transportation, and manufacturing. Others caution that there are also challenges and risks associated with the development of advanced AI, including ethical concerns, job displacement, and security issues.\n",
      "\n",
      "One trend that is likely to continue is the integration of AI into everyday life, making it more convenient and efficient for individuals and businesses. For example, we may see more widespread use of AI-powered virtual assistants, smart homes, and self-driving cars.\n",
      "\n",
      "Another area of focus is the development of more advanced forms of AI, such as deep learning and neural networks, which can learn and adapt to new situations on their own. This could lead to significant advances in areas such as natural language processing, computer vision, and robotics.\n",
      "\n",
      "However, there are also challenges and risks associated with the development of advanced AI. For example, there are concerns about the potential for AI to replace human workers, particularly in industries such as manufacturing and transportation. There are also ethical concerns about the use of AI\n"
     ]
    }
   ],
   "source": [
    "messages = [\n",
    "    {\"role\": \" user\", \"content\": test_prompt},\n",
    "]\n",
    "prompt = pipeline.tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "print(\"PROMPT:\\n\", prompt)\n",
    "outputs = pipeline(prompt, max_new_tokens=256, do_sample=True, temperature=0.01, top_k=50, top_p=.95)\n",
    "print(outputs[0][\"generated_text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PROMPT:\n",
      " prompt\n"
     ]
    }
   ],
   "source": [
    "print(\"PROMPT:\\n\", \"prompt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### It is critical to use specific formatting of the must be copied character for character "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sosa.s/.local/lib/python3.10/site-packages/transformers/pipelines/base.py:1123: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
      "  warnings.warn(\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s>[INST] The future of AI is [/INST] The future of AI (Artificial Intelligence) is a topic of much debate and speculation among experts, researchers, and industry professionals. Some believe that AI will continue to advance at an exponential rate, leading to significant breakthroughs in areas such as healthcare, education, transportation, and manufacturing. Others caution that there are also challenges and risks associated with the development of advanced AI, including ethical concerns, job displacement, and security issues.\n",
      "\n",
      "One trend that is likely to continue is the integration of AI into everyday life, making it more convenient and efficient for individuals and businesses. For example, we may see more widespread use of AI-powered virtual assistants, smart homes, and self-driving cars.\n",
      "\n",
      "Another area of focus is the development of more advanced forms of AI, such as deep learning and neural networks, which can learn and adapt to new situations on their own. This could lead to significant advances in areas such as natural language processing, computer vision, and robotics.\n",
      "\n",
      "However, there are also challenges and risks associated with the development of advanced AI. For example, there are concerns about the potential for AI to replace human workers, particularly in industries such as manufacturing and transportation. There are also ethical concerns about the use of AI\n"
     ]
    }
   ],
   "source": [
    "outputs = pipeline(\"<s>[INST] The future of AI is [/INST]\", max_new_tokens=256, do_sample=True, temperature=0.01, top_k=1)\n",
    "print(outputs[0][\"generated_text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s> 1\n",
      "<s> 1\n",
      "[ 733\n",
      "INST 16289\n",
      "] 28793\n",
      "The 415\n",
      "future 3437\n",
      "of 302\n",
      "AI 16107\n",
      "is 349\n",
      "[ 733\n",
      "/ 28748\n",
      "INST 16289\n",
      "] 28793\n"
     ]
    }
   ],
   "source": [
    "prompt_test = \"<s>[INST] The future of AI is [/INST]\"\n",
    "# show me how this prompt is tokenized\n",
    "tokeized_prompt = tokenizer_instruct(prompt_test, return_tensors=\"pt\")\n",
    "# show me each token id as the token\n",
    "for token_id in tokeized_prompt[\"input_ids\"][0]:\n",
    "    print(tokenizer_instruct.decode(token_id.item()), token_id.item())\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PyTorch-2.0.1",
   "language": "python",
   "name": "pytorch-2.0.1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
