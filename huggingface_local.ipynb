{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Running models locally\n",
    "- Using Huggingface transformers library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import bfloat16\n",
    "import transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a9d8832bc0204ea4b22437d9751dc38c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "MistralForCausalLM(\n",
       "  (model): MistralModel(\n",
       "    (embed_tokens): Embedding(32000, 4096)\n",
       "    (layers): ModuleList(\n",
       "      (0-31): 32 x MistralDecoderLayer(\n",
       "        (self_attn): MistralAttention(\n",
       "          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "          (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (rotary_emb): MistralRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): MistralMLP(\n",
       "          (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "          (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "          (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): MistralRMSNorm()\n",
       "        (post_attention_layernorm): MistralRMSNorm()\n",
       "      )\n",
       "    )\n",
       "    (norm): MistralRMSNorm()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=4096, out_features=32000, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_id = \"mistralai/Mistral-7B-v0.1\"\n",
    "model = transformers.AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    trust_remote_code=True,\n",
    "    torch_dtype=bfloat16,\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "tokenizer = transformers.AutoTokenizer.from_pretrained(model_id)\n",
    "\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Controlling the ouput of a model\n",
    "## What will the output look like\n",
    "- vocab = [\"You\", \"will\", \"win\", \"lose\"]\n",
    "- input = \"You will\"\n",
    "- Outputs unnormalized predictions for tokens\n",
    "- We can then normalize it = [0.1,0.1,0.5,0.3]\n",
    "- We can make an informed decision ow what token to select.\n",
    "\n",
    "## Args\n",
    "- top_k: selects the k most probable tokens\n",
    "- top_p: selects all tokens with higher than p probability\n",
    "- temperature: selects from the top tokens with .1 favoring the highest probability tokens and .9 having little preference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_text = transformers.pipeline(\n",
    "    model=model, tokenizer=tokenizer,\n",
    "    return_full_text=False,  # Set this to True if using langchain\n",
    "    task=\"text-generation\",\n",
    "    temperature=0.1,  # Controls the randomness of outputs\n",
    "    top_p=0.15,  # Probability threshold for selecting tokens\n",
    "    top_k=2,  # Number of top tokens to consider (0 relies on top_p)\n",
    "    max_new_tokens=512,  # Limits the number of generated tokens\n",
    "    repetition_penalty=1.1  # Discourages repetitive outputs\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{align*}\n",
    "    p'_i = p_i^{(1/T)}\n",
    "\\end{align*}\n",
    "<!-- show image temperature_ex.JPG -->\n",
    "<img src=\"temperature_ex.JPG\"\n",
    "     alt=\"Markdown Monster icon\"\n",
    "     style=\"float: left; margin-right: 10px;\" />\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sosa.s/.local/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:392: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "/home/sosa.s/.local/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:397: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.15` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n",
      "/home/sosa.s/.local/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:407: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `2` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.\n",
      "  warnings.warn(\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Generated text:\n",
      " bright, and it’s only going to get brighter.\n",
      "\n",
      "## What are the benefits of using AI in business?\n",
      "\n",
      "There are many benefits of using AI in business. Some of these benefits include:\n",
      "\n",
      "1. Increased efficiency and productivity: AI can help businesses automate tasks and processes, which can lead to increased efficiency and productivity.\n",
      "2. Improved decision-making: AI can help businesses make better decisions by providing them with data and insights that they wouldn’t otherwise have access to.\n",
      "3. Enhanced customer experience: AI can help businesses provide a more personalized and tailored experience for their customers.\n",
      "4. Reduced costs: AI can help businesses reduce costs by automating tasks and processes, as well as by making better decisions.\n",
      "5. Competitive advantage: By using AI, businesses can gain a competitive advantage over their competitors.\n",
      "\n",
      "## How can you use AI to improve your business?\n",
      "\n",
      "AI can be used to improve your business in a number of ways. For example, you can use AI to automate tasks, such as customer service or marketing. You can also use AI to analyze data and make predictions about your business. Additionally, AI can be used to create new products or services.\n",
      "\n",
      "## What are some of the challenges of using AI in business?\n",
      "\n",
      "Some of the challenges of using AI in business include:\n",
      "\n",
      "1. Lack of understanding of how AI works: Many people don’t understand how AI works, which can make it difficult to implement in a business setting.\n",
      "2. Data quality issues: In order for AI to work effectively, it needs high-quality data. If the data is not accurate or complete, then the AI will not be able to produce accurate results.\n",
      "3. Ethical concerns: There are ethical concerns surrounding the use of AI, such as privacy and bias. These concerns need to be addressed before implementing AI in a business setting.\n",
      "4. Cost: Implementing AI can be expensive, especially if you need to hire experts to help you set up and maintain the system.\n",
      "5. Security risks: AI systems can be vulnerable to security threats, such as hacking and malware attacks. This is something that businesses need to consider when deciding whether or not to use AI.\n",
      "\n",
      "## What are some of the best practices for using AI in business?\n",
      "\n",
      "There are a few key things to keep in mind when using AI in business. First, it’s important to have\n"
     ]
    }
   ],
   "source": [
    "test_prompt = \"The future of AI is\"\n",
    "result = generate_text(test_prompt)\n",
    "print(\"\\n\\nGenerated text:\\n\" + result[0]['generated_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "48f72884140e4e34999ecaea10bd6981",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model_instruct = \"mistralai/Mistral-7B-Instruct-v0.2\"\n",
    "\n",
    "tokenizer_instruct = transformers.AutoTokenizer.from_pretrained(model_instruct)\n",
    "pipeline = transformers.pipeline(\n",
    "    model=model_instruct,\n",
    "    task=\"text-generation\",\n",
    "    model_kwargs={\"torch_dtype\": bfloat16, \"device_map\": \"auto\"},\n",
    ")   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### It is critical to use specific formatting of the must be copied character for character "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "PROMPT:\n",
      " <s>[INST] The future of AI is [/INST]\n",
      "\n",
      "\n",
      "Generated response:\n",
      "<s>[INST] The future of AI is [/INST] The future of AI (Artificial Intelligence) is a topic of much debate and speculation among experts, researchers, and industry professionals. Some believe that AI will continue to advance at an exponential rate, leading to significant breakthroughs in areas such as healthcare, education, transportation, and manufacturing. Others caution that there are also risks and challenges associated with the development of advanced AI, including ethical concerns, job displacement, and security issues.\n",
      "\n",
      "One trend that is likely to continue is the integration of AI into everyday life, making it more convenient and efficient for individuals and businesses. For example, we may see more widespread use of AI-powered virtual assistants, smart homes, and self-driving cars.\n",
      "\n",
      "Another area of focus is the development of more advanced forms of AI, such as deep learning and neural networks, which can learn and adapt to new situations on their own. This could lead to significant advances in areas such as natural language processing, computer vision, and robotics.\n",
      "\n",
      "However, there are also challenges and risks associated with the development of advanced AI. For example, there are concerns about the potential for AI to replace human workers, particularly in industries such as manufacturing and transportation. There are also ethical concerns about the use of AI\n"
     ]
    }
   ],
   "source": [
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": test_prompt},\n",
    "]\n",
    "prompt = pipeline.tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "# prompt = \"<s>[INST] The future of AI is [/INST]\"\n",
    "print(\"\\n\\nPROMPT:\\n\", prompt)\n",
    "outputs = pipeline(prompt, max_new_tokens=256, do_sample=True, temperature=0.01, top_k=50, top_p=.95)\n",
    "print(\"\\n\\nGenerated response:\\n\" + outputs[0][\"generated_text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Generated response:\n",
      "The future of AI is not just about making machines smarter, but also about making humans smarter.\n",
      "\n",
      "AI is already being used in various industries to improve productivity, reduce costs, and enhance customer experiences. However, the true potential of AI lies in its ability to augment human intelligence and creativity. By combining human expertise with AI capabilities, we can create new solutions and innovations that were previously impossible.\n",
      "\n",
      "One example of this is in the field of healthcare. AI algorithms can analyze vast amounts of medical data to identify patterns and make predictions about patient outcomes. However, they cannot replace the human touch and empathy that is essential in providing quality healthcare. By integrating AI with human healthcare professionals, we can create a more personalized and effective healthcare system.\n",
      "\n",
      "Another example is in the field of education. AI can be used to personalize learning experiences based on individual student needs and preferences. However, it cannot replace the role of a skilled teacher in inspiring and motivating students. By combining AI with human teachers, we can create a more effective and engaging learning environment.\n",
      "\n",
      "In the future, AI will continue to evolve and become more integrated into our daily lives. However, it is important to remember that AI is a tool, not a replacement for human intelligence and\n"
     ]
    }
   ],
   "source": [
    "outputs = pipeline(test_prompt, max_new_tokens=256, do_sample=True, temperature=0.1, top_k=50, top_p=.95)\n",
    "print(\"\\n\\nGenerated response:\\n\" + outputs[0][\"generated_text\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- following the instruct structure is critical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s>  [INST] The future of AI is [/INST] a topic of great interest and debate among experts in the field of artificial intelligence (AI). Some believe that AI will continue to advance at an exponential rate, leading to significant breakthroughs in areas such as autonomous vehicles, healthcare diagnostics, and scientific research. Others are more cautious, raising concerns about the ethical implications of advanced AI, such as job displacement, privacy, and potential misuse.\n",
      "\n",
      "One thing is certain: AI is already having a profound impact on our lives, and its role will only continue to grow in the future. Some possible developments include:\n",
      "\n",
      "1. Increased automation: AI is already being used to automate many tasks, from manufacturing to customer service. In the future, we can expect to see even more automation, particularly in industries where labor is expensive or dangerous.\n",
      "2. Improved decision-making: AI is being used to analyze vast amounts of data and make decisions based on that data. In the future, we can expect to see AI making increasingly complex decisions, from managing financial portfolios to diagnosing medical conditions.\n",
      "3. Enhanced creativity: AI is being used to generate art, music, and even poetry. In the future, we can expect to see AI creating even\n"
     ]
    }
   ],
   "source": [
    "fail_test = \"<s>  [INST] The future of AI is [/INST]\"\n",
    "outputs = pipeline(fail_test, max_new_tokens=256, do_sample=True, temperature=0.01, top_k=1)\n",
    "print(outputs[0][\"generated_text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s> 1\n",
      "<s> 1\n",
      "  259\n",
      "[ 733\n",
      "INST 16289\n",
      "] 28793\n",
      "The 415\n",
      "future 3437\n",
      "of 302\n",
      "AI 16107\n",
      "is 349\n",
      "[ 733\n",
      "/ 28748\n",
      "INST 16289\n",
      "] 28793\n",
      "\n",
      "\n",
      "\n",
      "<s> 1\n",
      "<s> 1\n",
      "[ 733\n",
      "INST 16289\n",
      "] 28793\n",
      "The 415\n",
      "future 3437\n",
      "of 302\n",
      "AI 16107\n",
      "is 349\n",
      "[ 733\n",
      "/ 28748\n",
      "INST 16289\n",
      "] 28793\n"
     ]
    }
   ],
   "source": [
    "# show me how this prompt is tokenized\n",
    "tokeized_prompt = tokenizer_instruct(fail_test, return_tensors=\"pt\")\n",
    "# show me each token id as the token\n",
    "for token_id in tokeized_prompt[\"input_ids\"][0]:\n",
    "    print(tokenizer_instruct.decode(token_id.item()), token_id.item())\n",
    "print(\"\\n\\n\")\n",
    "\n",
    "tokeized_prompt = tokenizer_instruct(prompt, return_tensors=\"pt\")\n",
    "# show me each token id as the token\n",
    "for token_id in tokeized_prompt[\"input_ids\"][0]:\n",
    "    print(tokenizer_instruct.decode(token_id.item()), token_id.item())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PyTorch-2.0.1",
   "language": "python",
   "name": "pytorch-2.0.1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
