{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Running models locally\n",
    "- Using Huggingface transformers library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import bfloat16\n",
    "import transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "91c7b61146824de2a856caeb2d36aafc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "MistralForCausalLM(\n",
       "  (model): MistralModel(\n",
       "    (embed_tokens): Embedding(32000, 4096)\n",
       "    (layers): ModuleList(\n",
       "      (0-31): 32 x MistralDecoderLayer(\n",
       "        (self_attn): MistralAttention(\n",
       "          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "          (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (rotary_emb): MistralRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): MistralMLP(\n",
       "          (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "          (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "          (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): MistralRMSNorm()\n",
       "        (post_attention_layernorm): MistralRMSNorm()\n",
       "      )\n",
       "    )\n",
       "    (norm): MistralRMSNorm()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=4096, out_features=32000, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_id = \"mistralai/Mistral-7B-v0.1\"\n",
    "model = transformers.AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    trust_remote_code=True,\n",
    "    torch_dtype=bfloat16,\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "tokenizer = transformers.AutoTokenizer.from_pretrained(model_id)\n",
    "\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Controlling the ouput of a model\n",
    "## What will the output look like\n",
    "- vocab = [\"You\", \"will\", \"win\", \"lose\"]\n",
    "- input = \"You will\"\n",
    "- Outputs unnormalized predictions for tokens\n",
    "- We can then normalize it = [0.1,0.1,0.5,0.3]\n",
    "- We can make an informed decision ow what token to select.\n",
    "\n",
    "## Args\n",
    "- top_k: selects the k most probable tokens\n",
    "- top_p: selects all tokens with higher than p probability\n",
    "- temperature: selects from the top tokens with .1 favoring the highest probability tokens and .9 having little preference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_text = transformers.pipeline(\n",
    "    model=model, tokenizer=tokenizer,\n",
    "    return_full_text=False,  # Set this to True if using langchain\n",
    "    task=\"text-generation\",\n",
    "    temperature=0.1,  # Controls the randomness of outputs\n",
    "    top_p=0.15,  # Probability threshold for selecting tokens\n",
    "    top_k=2,  # Number of top tokens to consider (0 relies on top_p)\n",
    "    max_new_tokens=512,  # Limits the number of generated tokens\n",
    "    repetition_penalty=1.1  # Discourages repetitive outputs\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{align*}\n",
    "    p'_i = p_i^{(1/T)}\n",
    "\\end{align*}\n",
    "<!-- show image temperature_ex.JPG -->\n",
    "<img src=\"temperature_ex.JPG\"\n",
    "     alt=\"Markdown Monster icon\"\n",
    "     style=\"float: left; margin-right: 10px;\" />\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sosa.s/.local/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:392: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "/home/sosa.s/.local/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:397: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.15` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n",
      "/home/sosa.s/.local/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:407: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `2` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.\n",
      "  warnings.warn(\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Generated text:\n",
      " bright, and it’s only going to get brighter.\n",
      "\n",
      "## What are the benefits of using AI in business?\n",
      "\n",
      "There are many benefits of using AI in business. Some of these benefits include:\n",
      "\n",
      "1. Increased efficiency and productivity: AI can help businesses automate tasks and processes, which can lead to increased efficiency and productivity.\n",
      "2. Improved decision-making: AI can help businesses make better decisions by providing them with data and insights that they wouldn’t otherwise have access to.\n",
      "3. Enhanced customer experience: AI can help businesses provide a more personalized and tailored experience for their customers.\n",
      "4. Reduced costs: AI can help businesses reduce costs by automating tasks and processes, as well as by making better decisions.\n",
      "5. Competitive advantage: By using AI, businesses can gain a competitive advantage over their competitors.\n",
      "\n",
      "## How can you use AI to improve your business?\n",
      "\n",
      "AI can be used to improve your business in a number of ways. For example, you can use AI to automate tasks, such as customer service or marketing. You can also use AI to analyze data and make predictions about your business. Additionally, AI can be used to create new products or services.\n",
      "\n",
      "## What are some of the challenges of using AI in business?\n",
      "\n",
      "Some of the challenges of using AI in business include:\n",
      "\n",
      "1. Lack of understanding of how AI works: Many people don’t understand how AI works, which can make it difficult to implement in a business setting.\n",
      "2. Data quality issues: In order for AI to work effectively, it needs high-quality data. If the data is not accurate or complete, then the AI will not be able to produce accurate results.\n",
      "3. Ethical concerns: There are ethical concerns surrounding the use of AI, such as privacy and bias. These concerns need to be addressed before implementing AI in a business setting.\n",
      "4. Cost: Implementing AI can be expensive, especially if you need to hire experts to help you set up and maintain the system.\n",
      "5. Security risks: AI systems can be vulnerable to security threats, such as hacking and malware attacks. This is something that businesses need to consider when deciding whether or not to use AI.\n",
      "\n",
      "## What are some of the best practices for using AI in business?\n",
      "\n",
      "There are a few key things to keep in mind when using AI in business. First, it’s important to have\n"
     ]
    }
   ],
   "source": [
    "test_prompt = \"The future of AI is\"\n",
    "result = generate_text(test_prompt)\n",
    "print(\"\\n\\nGenerated text:\\n\" + result[0]['generated_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "159ff6d318314a2fba003c67e2e8258f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model_instruct = \"mistralai/Mistral-7B-Instruct-v0.2\"\n",
    "\n",
    "tokenizer_instruct = transformers.AutoTokenizer.from_pretrained(model_instruct)\n",
    "pipeline = transformers.pipeline(\n",
    "    model=model_instruct,\n",
    "    task=\"text-generation\",\n",
    "    model_kwargs={\"torch_dtype\": bfloat16, \"device_map\": \"auto\"},\n",
    ")   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### It is critical to use specific formatting of the must be copied character for character "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "PROMPT:\n",
      " <s>[INST] The future of AI is [/INST]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Generated response:\n",
      "<s>[INST] The future of AI is [/INST] The future of AI (Artificial Intelligence) is a topic of much debate and speculation among experts, researchers, and industry professionals. Some believe that AI will continue to advance at an exponential rate, leading to significant breakthroughs in areas such as healthcare, education, transportation, and manufacturing. Others caution that there are also risks and challenges associated with the development of advanced AI, including ethical concerns, job displacement, and security issues.\n",
      "\n",
      "One trend that is likely to continue is the integration of AI into everyday life, making it more convenient and efficient for individuals and businesses. For example, we may see more widespread use of AI-powered virtual assistants, smart homes, and self-driving cars.\n",
      "\n",
      "Another area of focus is the development of more advanced forms of AI, such as deep learning and neural networks, which can learn and adapt to new situations on their own. This could lead to significant advances in areas such as natural language processing, computer vision, and robotics.\n",
      "\n",
      "However, there are also challenges and risks associated with the development of advanced AI. For example, there are concerns about the potential for AI to replace human workers, particularly in industries such as manufacturing and transportation. There are also ethical concerns about the use of AI\n"
     ]
    }
   ],
   "source": [
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": test_prompt},\n",
    "]\n",
    "prompt = pipeline.tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "# prompt = \"<s>[INST] The future of AI is [/INST]\"\n",
    "print(\"\\n\\nPROMPT:\\n\", prompt)\n",
    "outputs = pipeline(prompt, max_new_tokens=256, do_sample=True, temperature=0.01, top_k=50, top_p=.95)\n",
    "print(\"\\n\\nGenerated response:\\n\" + outputs[0][\"generated_text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Generated response:\n",
      "The future of AI is not just about making machines smarter, but also about making them more human.\n",
      "\n",
      "## 10. The Future of AI: Ethics and Morality\n",
      "\n",
      "As AI becomes more advanced and integrated into our lives, ethical and moral considerations become increasingly important. Here are some of the key issues that need to be addressed:\n",
      "\n",
      "* **Bias and discrimination:** AI systems can reflect and perpetuate biases and discrimination, which can have negative consequences for individuals and society as a whole. It is important to ensure that AI systems are designed and trained in a way that is fair and unbiased.\n",
      "* **Privacy and security:** AI systems can collect and process vast amounts of personal data, raising concerns about privacy and security. It is important to ensure that AI systems are designed with strong privacy and security protections, and that individuals have control over their own data.\n",
      "* **Transparency and explainability:** AI systems can be complex and difficult to understand, making it challenging to determine how they make decisions and why. It is important to ensure that AI systems are transparent and explainable, so that individuals can understand how they are being impacted and can hold them accountable.\n",
      "* **Accountability and responsibility:** As AI systems become more\n"
     ]
    }
   ],
   "source": [
    "outputs = pipeline(test_prompt, max_new_tokens=256, do_sample=True, temperature=0.1, top_k=50, top_p=.95)\n",
    "print(\"\\n\\nGenerated response:\\n\" + outputs[0][\"generated_text\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- following the instruct structure is critical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s>  [INST] The future of AI is [/INST] a topic of great interest and debate among experts in the field of artificial intelligence (AI). Some believe that AI will continue to advance at an exponential rate, leading to significant breakthroughs in areas such as autonomous vehicles, healthcare diagnostics, and scientific research. Others are more cautious, raising concerns about the ethical implications of advanced AI, such as job displacement, privacy invasion, and potential misuse.\n",
      "\n",
      "One thing is certain: AI is already having a profound impact on our lives, and its role will only continue to grow in the future. Some possible developments include:\n",
      "\n",
      "* Increased automation of repetitive and mundane tasks, freeing up humans to focus on more creative and complex problem-solving.\n",
      "* Improved accuracy and efficiency in areas such as healthcare diagnostics, financial analysis, and customer service.\n",
      "* Advances in machine learning and deep learning, enabling AI systems to learn and adapt to new situations more effectively.\n",
      "* Greater integration of AI into everyday life, from smart homes and wearable devices to virtual assistants and autonomous vehicles.\n",
      "* New applications of AI in fields such as education, entertainment, and art.\n",
      "\n",
      "However, as AI becomes more advanced and ubiquitous,\n"
     ]
    }
   ],
   "source": [
    "fail_test = \"<s>  [INST] The future of AI is [/INST]\"\n",
    "outputs = pipeline(fail_test, max_new_tokens=256, do_sample=True, temperature=0.01, top_k=1)\n",
    "print(outputs[0][\"generated_text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s> 1\n",
      "<s> 1\n",
      "  259\n",
      "[ 733\n",
      "INST 16289\n",
      "] 28793\n",
      "The 415\n",
      "future 3437\n",
      "of 302\n",
      "AI 16107\n",
      "is 349\n",
      "[ 733\n",
      "/ 28748\n",
      "INST 16289\n",
      "] 28793\n",
      "\n",
      "\n",
      "\n",
      "<s> 1\n",
      "<s> 1\n",
      "[ 733\n",
      "INST 16289\n",
      "] 28793\n",
      "The 415\n",
      "future 3437\n",
      "of 302\n",
      "AI 16107\n",
      "is 349\n",
      "[ 733\n",
      "/ 28748\n",
      "INST 16289\n",
      "] 28793\n"
     ]
    }
   ],
   "source": [
    "# show me how this prompt is tokenized\n",
    "tokeized_prompt = tokenizer_instruct(fail_test, return_tensors=\"pt\")\n",
    "# show me each token id as the token\n",
    "for token_id in tokeized_prompt[\"input_ids\"][0]:\n",
    "    print(tokenizer_instruct.decode(token_id.item()), token_id.item())\n",
    "print(\"\\n\\n\")\n",
    "\n",
    "tokeized_prompt = tokenizer_instruct(prompt, return_tensors=\"pt\")\n",
    "# show me each token id as the token\n",
    "for token_id in tokeized_prompt[\"input_ids\"][0]:\n",
    "    print(tokenizer_instruct.decode(token_id.item()), token_id.item())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PyTorch-2.0.1",
   "language": "python",
   "name": "pytorch-2.0.1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
